{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#KERAS\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score for Random Forest Model is 95.000\n",
      "\n",
      "Random Forest roc-auc measure is 0.985\n",
      "\n",
      "Random Forest  Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97        93\n",
      "         1.0       0.67      0.57      0.62         7\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       100\n",
      "   macro avg       0.82      0.77      0.79       100\n",
      "weighted avg       0.95      0.95      0.95       100\n",
      "\n",
      "\n",
      "************Start of Nerual Netwrok Summary*********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 85\n",
      "Trainable params: 85\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1222 14:32:46.105570 17996 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 232 samples, validate on 100 samples\n",
      "Epoch 1/200\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6351 - acc: 0.6422 - val_loss: 0.6299 - val_acc: 0.5900\n",
      "Epoch 2/200\n",
      "232/232 [==============================] - 0s 171us/step - loss: 0.6262 - acc: 0.6422 - val_loss: 0.6202 - val_acc: 0.6100\n",
      "Epoch 3/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.6176 - acc: 0.6422 - val_loss: 0.6110 - val_acc: 0.6400\n",
      "Epoch 4/200\n",
      "232/232 [==============================] - 0s 185us/step - loss: 0.6094 - acc: 0.6595 - val_loss: 0.6026 - val_acc: 0.6400\n",
      "Epoch 5/200\n",
      "232/232 [==============================] - 0s 183us/step - loss: 0.6019 - acc: 0.6595 - val_loss: 0.5940 - val_acc: 0.6400\n",
      "Epoch 6/200\n",
      "232/232 [==============================] - 0s 175us/step - loss: 0.5944 - acc: 0.6810 - val_loss: 0.5857 - val_acc: 0.6600\n",
      "Epoch 7/200\n",
      "232/232 [==============================] - 0s 179us/step - loss: 0.5870 - acc: 0.6897 - val_loss: 0.5776 - val_acc: 0.6600\n",
      "Epoch 8/200\n",
      "232/232 [==============================] - 0s 176us/step - loss: 0.5798 - acc: 0.6940 - val_loss: 0.5698 - val_acc: 0.6600\n",
      "Epoch 9/200\n",
      "232/232 [==============================] - 0s 187us/step - loss: 0.5730 - acc: 0.6940 - val_loss: 0.5622 - val_acc: 0.6600\n",
      "Epoch 10/200\n",
      "232/232 [==============================] - 0s 184us/step - loss: 0.5664 - acc: 0.6983 - val_loss: 0.5550 - val_acc: 0.6800\n",
      "Epoch 11/200\n",
      "232/232 [==============================] - 0s 183us/step - loss: 0.5600 - acc: 0.7026 - val_loss: 0.5475 - val_acc: 0.7000\n",
      "Epoch 12/200\n",
      "232/232 [==============================] - 0s 171us/step - loss: 0.5533 - acc: 0.7155 - val_loss: 0.5403 - val_acc: 0.7000\n",
      "Epoch 13/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.5470 - acc: 0.7198 - val_loss: 0.5331 - val_acc: 0.7200\n",
      "Epoch 14/200\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.5406 - acc: 0.7371 - val_loss: 0.5267 - val_acc: 0.7200\n",
      "Epoch 15/200\n",
      "232/232 [==============================] - 0s 154us/step - loss: 0.5349 - acc: 0.7543 - val_loss: 0.5203 - val_acc: 0.7400\n",
      "Epoch 16/200\n",
      "232/232 [==============================] - 0s 160us/step - loss: 0.5292 - acc: 0.7759 - val_loss: 0.5140 - val_acc: 0.8000\n",
      "Epoch 17/200\n",
      "232/232 [==============================] - 0s 180us/step - loss: 0.5237 - acc: 0.7888 - val_loss: 0.5077 - val_acc: 0.8000\n",
      "Epoch 18/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.5182 - acc: 0.7974 - val_loss: 0.5018 - val_acc: 0.8000\n",
      "Epoch 19/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.5129 - acc: 0.8060 - val_loss: 0.4961 - val_acc: 0.8000\n",
      "Epoch 20/200\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.5078 - acc: 0.8060 - val_loss: 0.4902 - val_acc: 0.8000\n",
      "Epoch 21/200\n",
      "232/232 [==============================] - 0s 197us/step - loss: 0.5026 - acc: 0.8276 - val_loss: 0.4848 - val_acc: 0.8400\n",
      "Epoch 22/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.4978 - acc: 0.8319 - val_loss: 0.4793 - val_acc: 0.8600\n",
      "Epoch 23/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.4929 - acc: 0.8405 - val_loss: 0.4739 - val_acc: 0.8600\n",
      "Epoch 24/200\n",
      "232/232 [==============================] - 0s 208us/step - loss: 0.4881 - acc: 0.8448 - val_loss: 0.4690 - val_acc: 0.8600\n",
      "Epoch 25/200\n",
      "232/232 [==============================] - 0s 166us/step - loss: 0.4838 - acc: 0.8491 - val_loss: 0.4640 - val_acc: 0.8600\n",
      "Epoch 26/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.4794 - acc: 0.8491 - val_loss: 0.4589 - val_acc: 0.8600\n",
      "Epoch 27/200\n",
      "232/232 [==============================] - 0s 185us/step - loss: 0.4749 - acc: 0.8534 - val_loss: 0.4539 - val_acc: 0.8700\n",
      "Epoch 28/200\n",
      "232/232 [==============================] - 0s 199us/step - loss: 0.4705 - acc: 0.8707 - val_loss: 0.4493 - val_acc: 0.8900\n",
      "Epoch 29/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.4664 - acc: 0.8750 - val_loss: 0.4448 - val_acc: 0.9000\n",
      "Epoch 30/200\n",
      "232/232 [==============================] - 0s 187us/step - loss: 0.4625 - acc: 0.8922 - val_loss: 0.4402 - val_acc: 0.9000\n",
      "Epoch 31/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.4584 - acc: 0.8966 - val_loss: 0.4358 - val_acc: 0.9000\n",
      "Epoch 32/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.4546 - acc: 0.8966 - val_loss: 0.4314 - val_acc: 0.9100\n",
      "Epoch 33/200\n",
      "232/232 [==============================] - 0s 162us/step - loss: 0.4507 - acc: 0.8966 - val_loss: 0.4272 - val_acc: 0.9100\n",
      "Epoch 34/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.4469 - acc: 0.8966 - val_loss: 0.4232 - val_acc: 0.9100\n",
      "Epoch 35/200\n",
      "232/232 [==============================] - 0s 187us/step - loss: 0.4434 - acc: 0.8922 - val_loss: 0.4192 - val_acc: 0.9100\n",
      "Epoch 36/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.4400 - acc: 0.8922 - val_loss: 0.4152 - val_acc: 0.9200\n",
      "Epoch 37/200\n",
      "232/232 [==============================] - 0s 202us/step - loss: 0.4364 - acc: 0.8922 - val_loss: 0.4114 - val_acc: 0.9200\n",
      "Epoch 38/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.4331 - acc: 0.8922 - val_loss: 0.4076 - val_acc: 0.9200\n",
      "Epoch 39/200\n",
      "232/232 [==============================] - 0s 177us/step - loss: 0.4298 - acc: 0.8922 - val_loss: 0.4040 - val_acc: 0.9300\n",
      "Epoch 40/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.4267 - acc: 0.8922 - val_loss: 0.4004 - val_acc: 0.9300\n",
      "Epoch 41/200\n",
      "232/232 [==============================] - 0s 174us/step - loss: 0.4236 - acc: 0.8922 - val_loss: 0.3969 - val_acc: 0.9300\n",
      "Epoch 42/200\n",
      "232/232 [==============================] - 0s 175us/step - loss: 0.4205 - acc: 0.8922 - val_loss: 0.3932 - val_acc: 0.9300\n",
      "Epoch 43/200\n",
      "232/232 [==============================] - 0s 162us/step - loss: 0.4173 - acc: 0.8922 - val_loss: 0.3897 - val_acc: 0.9400\n",
      "Epoch 44/200\n",
      "232/232 [==============================] - 0s 174us/step - loss: 0.4143 - acc: 0.8922 - val_loss: 0.3863 - val_acc: 0.9500\n",
      "Epoch 45/200\n",
      "232/232 [==============================] - 0s 162us/step - loss: 0.4113 - acc: 0.8879 - val_loss: 0.3829 - val_acc: 0.9500\n",
      "Epoch 46/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.4084 - acc: 0.8879 - val_loss: 0.3797 - val_acc: 0.9500\n",
      "Epoch 47/200\n",
      "232/232 [==============================] - 0s 159us/step - loss: 0.4056 - acc: 0.8879 - val_loss: 0.3765 - val_acc: 0.9500\n",
      "Epoch 48/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.4029 - acc: 0.8879 - val_loss: 0.3734 - val_acc: 0.9500\n",
      "Epoch 49/200\n",
      "232/232 [==============================] - 0s 167us/step - loss: 0.4002 - acc: 0.8879 - val_loss: 0.3702 - val_acc: 0.9500\n",
      "Epoch 50/200\n",
      "232/232 [==============================] - 0s 174us/step - loss: 0.3975 - acc: 0.8879 - val_loss: 0.3674 - val_acc: 0.9500\n",
      "Epoch 51/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.3950 - acc: 0.8879 - val_loss: 0.3644 - val_acc: 0.9500\n",
      "Epoch 52/200\n",
      "232/232 [==============================] - 0s 170us/step - loss: 0.3925 - acc: 0.8879 - val_loss: 0.3616 - val_acc: 0.9500\n",
      "Epoch 53/200\n",
      "232/232 [==============================] - 0s 171us/step - loss: 0.3900 - acc: 0.8879 - val_loss: 0.3586 - val_acc: 0.9500\n",
      "Epoch 54/200\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.3875 - acc: 0.8879 - val_loss: 0.3559 - val_acc: 0.9500\n",
      "Epoch 55/200\n",
      "232/232 [==============================] - 0s 192us/step - loss: 0.3851 - acc: 0.8879 - val_loss: 0.3530 - val_acc: 0.9500\n",
      "Epoch 56/200\n",
      "232/232 [==============================] - 0s 185us/step - loss: 0.3826 - acc: 0.8879 - val_loss: 0.3502 - val_acc: 0.9500\n",
      "Epoch 57/200\n",
      "232/232 [==============================] - 0s 180us/step - loss: 0.3802 - acc: 0.8879 - val_loss: 0.3475 - val_acc: 0.9500\n",
      "Epoch 58/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.3779 - acc: 0.8879 - val_loss: 0.3449 - val_acc: 0.9500\n",
      "Epoch 59/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.3756 - acc: 0.8879 - val_loss: 0.3422 - val_acc: 0.9500\n",
      "Epoch 60/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.3733 - acc: 0.8879 - val_loss: 0.3398 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "232/232 [==============================] - 0s 203us/step - loss: 0.3713 - acc: 0.8879 - val_loss: 0.3373 - val_acc: 0.9500\n",
      "Epoch 62/200\n",
      "232/232 [==============================] - 0s 221us/step - loss: 0.3691 - acc: 0.8879 - val_loss: 0.3350 - val_acc: 0.9500\n",
      "Epoch 63/200\n",
      "232/232 [==============================] - 0s 183us/step - loss: 0.3671 - acc: 0.8879 - val_loss: 0.3326 - val_acc: 0.9500\n",
      "Epoch 64/200\n",
      "232/232 [==============================] - 0s 217us/step - loss: 0.3650 - acc: 0.8879 - val_loss: 0.3303 - val_acc: 0.9500\n",
      "Epoch 65/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.3630 - acc: 0.8879 - val_loss: 0.3280 - val_acc: 0.9500\n",
      "Epoch 66/200\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.3611 - acc: 0.8879 - val_loss: 0.3258 - val_acc: 0.9500\n",
      "Epoch 67/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.3591 - acc: 0.8879 - val_loss: 0.3235 - val_acc: 0.9500\n",
      "Epoch 68/200\n",
      "232/232 [==============================] - 0s 172us/step - loss: 0.3572 - acc: 0.8922 - val_loss: 0.3215 - val_acc: 0.9500\n",
      "Epoch 69/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.3554 - acc: 0.8922 - val_loss: 0.3193 - val_acc: 0.9500\n",
      "Epoch 70/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.3535 - acc: 0.8922 - val_loss: 0.3172 - val_acc: 0.9500\n",
      "Epoch 71/200\n",
      "232/232 [==============================] - 0s 216us/step - loss: 0.3517 - acc: 0.8922 - val_loss: 0.3151 - val_acc: 0.9500\n",
      "Epoch 72/200\n",
      "232/232 [==============================] - 0s 183us/step - loss: 0.3498 - acc: 0.8922 - val_loss: 0.3130 - val_acc: 0.9500\n",
      "Epoch 73/200\n",
      "232/232 [==============================] - 0s 202us/step - loss: 0.3480 - acc: 0.8966 - val_loss: 0.3109 - val_acc: 0.9500\n",
      "Epoch 74/200\n",
      "232/232 [==============================] - 0s 209us/step - loss: 0.3462 - acc: 0.8966 - val_loss: 0.3089 - val_acc: 0.9500\n",
      "Epoch 75/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.3445 - acc: 0.8966 - val_loss: 0.3071 - val_acc: 0.9500\n",
      "Epoch 76/200\n",
      "232/232 [==============================] - 0s 174us/step - loss: 0.3429 - acc: 0.8966 - val_loss: 0.3053 - val_acc: 0.9500\n",
      "Epoch 77/200\n",
      "232/232 [==============================] - 0s 211us/step - loss: 0.3413 - acc: 0.8966 - val_loss: 0.3033 - val_acc: 0.9500\n",
      "Epoch 78/200\n",
      "232/232 [==============================] - 0s 222us/step - loss: 0.3396 - acc: 0.8966 - val_loss: 0.3015 - val_acc: 0.9500\n",
      "Epoch 79/200\n",
      "232/232 [==============================] - 0s 210us/step - loss: 0.3381 - acc: 0.8966 - val_loss: 0.2996 - val_acc: 0.9400\n",
      "Epoch 80/200\n",
      "232/232 [==============================] - 0s 204us/step - loss: 0.3364 - acc: 0.8966 - val_loss: 0.2979 - val_acc: 0.9400\n",
      "Epoch 81/200\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.3349 - acc: 0.8966 - val_loss: 0.2960 - val_acc: 0.9400\n",
      "Epoch 82/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.3333 - acc: 0.8966 - val_loss: 0.2943 - val_acc: 0.9400\n",
      "Epoch 83/200\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.3318 - acc: 0.8966 - val_loss: 0.2926 - val_acc: 0.9400\n",
      "Epoch 84/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.3304 - acc: 0.8966 - val_loss: 0.2910 - val_acc: 0.9400\n",
      "Epoch 85/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.3289 - acc: 0.8966 - val_loss: 0.2893 - val_acc: 0.9400\n",
      "Epoch 86/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.3275 - acc: 0.8966 - val_loss: 0.2878 - val_acc: 0.9400\n",
      "Epoch 87/200\n",
      "232/232 [==============================] - 0s 221us/step - loss: 0.3262 - acc: 0.8966 - val_loss: 0.2861 - val_acc: 0.9400\n",
      "Epoch 88/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.3247 - acc: 0.8966 - val_loss: 0.2845 - val_acc: 0.9400\n",
      "Epoch 89/200\n",
      "232/232 [==============================] - 0s 212us/step - loss: 0.3233 - acc: 0.8966 - val_loss: 0.2829 - val_acc: 0.9400\n",
      "Epoch 90/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.3218 - acc: 0.8966 - val_loss: 0.2814 - val_acc: 0.9400\n",
      "Epoch 91/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.3205 - acc: 0.8966 - val_loss: 0.2799 - val_acc: 0.9400\n",
      "Epoch 92/200\n",
      "232/232 [==============================] - 0s 216us/step - loss: 0.3192 - acc: 0.8966 - val_loss: 0.2785 - val_acc: 0.9400\n",
      "Epoch 93/200\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.3179 - acc: 0.8966 - val_loss: 0.2770 - val_acc: 0.9400\n",
      "Epoch 94/200\n",
      "232/232 [==============================] - 0s 187us/step - loss: 0.3166 - acc: 0.8966 - val_loss: 0.2755 - val_acc: 0.9400\n",
      "Epoch 95/200\n",
      "232/232 [==============================] - 0s 204us/step - loss: 0.3153 - acc: 0.8966 - val_loss: 0.2740 - val_acc: 0.9400\n",
      "Epoch 96/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.3141 - acc: 0.8966 - val_loss: 0.2726 - val_acc: 0.9400\n",
      "Epoch 97/200\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.3128 - acc: 0.8966 - val_loss: 0.2712 - val_acc: 0.9400\n",
      "Epoch 98/200\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.3115 - acc: 0.8966 - val_loss: 0.2698 - val_acc: 0.9400\n",
      "Epoch 99/200\n",
      "232/232 [==============================] - 0s 202us/step - loss: 0.3104 - acc: 0.9009 - val_loss: 0.2685 - val_acc: 0.9400\n",
      "Epoch 100/200\n",
      "232/232 [==============================] - 0s 206us/step - loss: 0.3092 - acc: 0.9009 - val_loss: 0.2672 - val_acc: 0.9400\n",
      "Epoch 101/200\n",
      "232/232 [==============================] - 0s 216us/step - loss: 0.3081 - acc: 0.9009 - val_loss: 0.2658 - val_acc: 0.9400\n",
      "Epoch 102/200\n",
      "232/232 [==============================] - 0s 221us/step - loss: 0.3069 - acc: 0.9009 - val_loss: 0.2645 - val_acc: 0.9400\n",
      "Epoch 103/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.3057 - acc: 0.9009 - val_loss: 0.2632 - val_acc: 0.9400\n",
      "Epoch 104/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.3046 - acc: 0.9009 - val_loss: 0.2619 - val_acc: 0.9400\n",
      "Epoch 105/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.3035 - acc: 0.9009 - val_loss: 0.2607 - val_acc: 0.9400\n",
      "Epoch 106/200\n",
      "232/232 [==============================] - 0s 167us/step - loss: 0.3024 - acc: 0.9009 - val_loss: 0.2594 - val_acc: 0.9400\n",
      "Epoch 107/200\n",
      "232/232 [==============================] - 0s 180us/step - loss: 0.3013 - acc: 0.9009 - val_loss: 0.2583 - val_acc: 0.9400\n",
      "Epoch 108/200\n",
      "232/232 [==============================] - 0s 179us/step - loss: 0.3003 - acc: 0.9009 - val_loss: 0.2571 - val_acc: 0.9400\n",
      "Epoch 109/200\n",
      "232/232 [==============================] - 0s 161us/step - loss: 0.2993 - acc: 0.9009 - val_loss: 0.2559 - val_acc: 0.9400\n",
      "Epoch 110/200\n",
      "232/232 [==============================] - 0s 164us/step - loss: 0.2982 - acc: 0.9009 - val_loss: 0.2548 - val_acc: 0.9400\n",
      "Epoch 111/200\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.3477 - acc: 0.875 - 0s 168us/step - loss: 0.2973 - acc: 0.9009 - val_loss: 0.2537 - val_acc: 0.9400\n",
      "Epoch 112/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.2963 - acc: 0.9009 - val_loss: 0.2526 - val_acc: 0.9400\n",
      "Epoch 113/200\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.2954 - acc: 0.9009 - val_loss: 0.2515 - val_acc: 0.9400\n",
      "Epoch 114/200\n",
      "232/232 [==============================] - 0s 172us/step - loss: 0.2944 - acc: 0.9009 - val_loss: 0.2504 - val_acc: 0.9400\n",
      "Epoch 115/200\n",
      "232/232 [==============================] - 0s 167us/step - loss: 0.2934 - acc: 0.9009 - val_loss: 0.2494 - val_acc: 0.9400\n",
      "Epoch 116/200\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.2925 - acc: 0.9009 - val_loss: 0.2483 - val_acc: 0.9400\n",
      "Epoch 117/200\n",
      "232/232 [==============================] - 0s 167us/step - loss: 0.2916 - acc: 0.9009 - val_loss: 0.2472 - val_acc: 0.9400\n",
      "Epoch 118/200\n",
      "232/232 [==============================] - 0s 156us/step - loss: 0.2907 - acc: 0.9009 - val_loss: 0.2462 - val_acc: 0.9400\n",
      "Epoch 119/200\n",
      "232/232 [==============================] - 0s 171us/step - loss: 0.2897 - acc: 0.9009 - val_loss: 0.2451 - val_acc: 0.9400\n",
      "Epoch 120/200\n",
      "232/232 [==============================] - 0s 175us/step - loss: 0.2888 - acc: 0.9009 - val_loss: 0.2441 - val_acc: 0.9400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.2879 - acc: 0.9009 - val_loss: 0.2432 - val_acc: 0.9400\n",
      "Epoch 122/200\n",
      "232/232 [==============================] - 0s 159us/step - loss: 0.2871 - acc: 0.9009 - val_loss: 0.2422 - val_acc: 0.9400\n",
      "Epoch 123/200\n",
      "232/232 [==============================] - 0s 158us/step - loss: 0.2862 - acc: 0.9009 - val_loss: 0.2412 - val_acc: 0.9400\n",
      "Epoch 124/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.2854 - acc: 0.9009 - val_loss: 0.2403 - val_acc: 0.9400\n",
      "Epoch 125/200\n",
      "232/232 [==============================] - 0s 171us/step - loss: 0.2845 - acc: 0.9009 - val_loss: 0.2394 - val_acc: 0.9400\n",
      "Epoch 126/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.2837 - acc: 0.9009 - val_loss: 0.2384 - val_acc: 0.9400\n",
      "Epoch 127/200\n",
      "232/232 [==============================] - 0s 207us/step - loss: 0.2829 - acc: 0.9009 - val_loss: 0.2375 - val_acc: 0.9400\n",
      "Epoch 128/200\n",
      "232/232 [==============================] - 0s 167us/step - loss: 0.2820 - acc: 0.9009 - val_loss: 0.2365 - val_acc: 0.9400\n",
      "Epoch 129/200\n",
      "232/232 [==============================] - 0s 170us/step - loss: 0.2813 - acc: 0.9009 - val_loss: 0.2356 - val_acc: 0.9400\n",
      "Epoch 130/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.2804 - acc: 0.9009 - val_loss: 0.2348 - val_acc: 0.9400\n",
      "Epoch 131/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.2796 - acc: 0.9009 - val_loss: 0.2339 - val_acc: 0.9400\n",
      "Epoch 132/200\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.1697 - acc: 1.000 - 0s 168us/step - loss: 0.2789 - acc: 0.9009 - val_loss: 0.2330 - val_acc: 0.9400\n",
      "Epoch 133/200\n",
      "232/232 [==============================] - 0s 179us/step - loss: 0.2781 - acc: 0.9009 - val_loss: 0.2321 - val_acc: 0.9400\n",
      "Epoch 134/200\n",
      "232/232 [==============================] - 0s 160us/step - loss: 0.2773 - acc: 0.9009 - val_loss: 0.2313 - val_acc: 0.9400\n",
      "Epoch 135/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.2766 - acc: 0.8966 - val_loss: 0.2304 - val_acc: 0.9400\n",
      "Epoch 136/200\n",
      "232/232 [==============================] - 0s 175us/step - loss: 0.2758 - acc: 0.9009 - val_loss: 0.2296 - val_acc: 0.9400\n",
      "Epoch 137/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.2751 - acc: 0.8966 - val_loss: 0.2288 - val_acc: 0.9400\n",
      "Epoch 138/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.2744 - acc: 0.8966 - val_loss: 0.2281 - val_acc: 0.9400\n",
      "Epoch 139/200\n",
      "232/232 [==============================] - 0s 177us/step - loss: 0.2737 - acc: 0.8966 - val_loss: 0.2272 - val_acc: 0.9400\n",
      "Epoch 140/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9052 - val_loss: 0.2264 - val_acc: 0.9400\n",
      "Epoch 141/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.2722 - acc: 0.9052 - val_loss: 0.2256 - val_acc: 0.9400\n",
      "Epoch 142/200\n",
      "232/232 [==============================] - 0s 208us/step - loss: 0.2715 - acc: 0.9052 - val_loss: 0.2248 - val_acc: 0.9400\n",
      "Epoch 143/200\n",
      "232/232 [==============================] - 0s 201us/step - loss: 0.2708 - acc: 0.9052 - val_loss: 0.2240 - val_acc: 0.9400\n",
      "Epoch 144/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.2701 - acc: 0.9052 - val_loss: 0.2232 - val_acc: 0.9400\n",
      "Epoch 145/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.2694 - acc: 0.9052 - val_loss: 0.2224 - val_acc: 0.9400\n",
      "Epoch 146/200\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.2687 - acc: 0.9052 - val_loss: 0.2217 - val_acc: 0.9400\n",
      "Epoch 147/200\n",
      "232/232 [==============================] - 0s 204us/step - loss: 0.2680 - acc: 0.9052 - val_loss: 0.2210 - val_acc: 0.9400\n",
      "Epoch 148/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.2674 - acc: 0.9052 - val_loss: 0.2202 - val_acc: 0.9400\n",
      "Epoch 149/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.2667 - acc: 0.9052 - val_loss: 0.2195 - val_acc: 0.9400\n",
      "Epoch 150/200\n",
      "232/232 [==============================] - 0s 197us/step - loss: 0.2661 - acc: 0.9052 - val_loss: 0.2188 - val_acc: 0.9400\n",
      "Epoch 151/200\n",
      "232/232 [==============================] - 0s 184us/step - loss: 0.2654 - acc: 0.9052 - val_loss: 0.2181 - val_acc: 0.9400\n",
      "Epoch 152/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.2648 - acc: 0.9052 - val_loss: 0.2174 - val_acc: 0.9400\n",
      "Epoch 153/200\n",
      "232/232 [==============================] - 0s 174us/step - loss: 0.2642 - acc: 0.9052 - val_loss: 0.2167 - val_acc: 0.9400\n",
      "Epoch 154/200\n",
      "232/232 [==============================] - 0s 177us/step - loss: 0.2636 - acc: 0.9052 - val_loss: 0.2160 - val_acc: 0.9400\n",
      "Epoch 155/200\n",
      "232/232 [==============================] - 0s 197us/step - loss: 0.2630 - acc: 0.9052 - val_loss: 0.2153 - val_acc: 0.9400\n",
      "Epoch 156/200\n",
      "232/232 [==============================] - 0s 216us/step - loss: 0.2624 - acc: 0.9052 - val_loss: 0.2147 - val_acc: 0.9400\n",
      "Epoch 157/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.2618 - acc: 0.9052 - val_loss: 0.2140 - val_acc: 0.9400\n",
      "Epoch 158/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.2612 - acc: 0.9052 - val_loss: 0.2133 - val_acc: 0.9400\n",
      "Epoch 159/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.2606 - acc: 0.9052 - val_loss: 0.2127 - val_acc: 0.9400\n",
      "Epoch 160/200\n",
      "232/232 [==============================] - 0s 199us/step - loss: 0.2600 - acc: 0.9052 - val_loss: 0.2120 - val_acc: 0.9400\n",
      "Epoch 161/200\n",
      "232/232 [==============================] - 0s 203us/step - loss: 0.2594 - acc: 0.9138 - val_loss: 0.2114 - val_acc: 0.9400\n",
      "Epoch 162/200\n",
      "232/232 [==============================] - 0s 189us/step - loss: 0.2588 - acc: 0.9138 - val_loss: 0.2108 - val_acc: 0.9400\n",
      "Epoch 163/200\n",
      "232/232 [==============================] - 0s 185us/step - loss: 0.2583 - acc: 0.9138 - val_loss: 0.2101 - val_acc: 0.9400\n",
      "Epoch 164/200\n",
      "232/232 [==============================] - 0s 202us/step - loss: 0.2577 - acc: 0.9138 - val_loss: 0.2095 - val_acc: 0.9400\n",
      "Epoch 165/200\n",
      "232/232 [==============================] - 0s 188us/step - loss: 0.2572 - acc: 0.9138 - val_loss: 0.2089 - val_acc: 0.9400\n",
      "Epoch 166/200\n",
      "232/232 [==============================] - 0s 169us/step - loss: 0.2566 - acc: 0.9138 - val_loss: 0.2083 - val_acc: 0.9400\n",
      "Epoch 167/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.2561 - acc: 0.9138 - val_loss: 0.2078 - val_acc: 0.9400\n",
      "Epoch 168/200\n",
      "232/232 [==============================] - 0s 206us/step - loss: 0.2556 - acc: 0.9138 - val_loss: 0.2071 - val_acc: 0.9400\n",
      "Epoch 169/200\n",
      "232/232 [==============================] - 0s 219us/step - loss: 0.2550 - acc: 0.9138 - val_loss: 0.2065 - val_acc: 0.9400\n",
      "Epoch 170/200\n",
      "232/232 [==============================] - 0s 202us/step - loss: 0.2545 - acc: 0.9138 - val_loss: 0.2059 - val_acc: 0.9400\n",
      "Epoch 171/200\n",
      "232/232 [==============================] - 0s 181us/step - loss: 0.2540 - acc: 0.9138 - val_loss: 0.2054 - val_acc: 0.9400\n",
      "Epoch 172/200\n",
      "232/232 [==============================] - 0s 197us/step - loss: 0.2534 - acc: 0.9138 - val_loss: 0.2048 - val_acc: 0.9400\n",
      "Epoch 173/200\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.2529 - acc: 0.9138 - val_loss: 0.2042 - val_acc: 0.9400\n",
      "Epoch 174/200\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.2524 - acc: 0.9138 - val_loss: 0.2036 - val_acc: 0.9400\n",
      "Epoch 175/200\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.2519 - acc: 0.9138 - val_loss: 0.2030 - val_acc: 0.9400\n",
      "Epoch 176/200\n",
      "232/232 [==============================] - 0s 170us/step - loss: 0.2514 - acc: 0.9138 - val_loss: 0.2025 - val_acc: 0.9400\n",
      "Epoch 177/200\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.2508 - acc: 0.9138 - val_loss: 0.2019 - val_acc: 0.9400\n",
      "Epoch 178/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.2504 - acc: 0.9138 - val_loss: 0.2014 - val_acc: 0.9400\n",
      "Epoch 179/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.2499 - acc: 0.9138 - val_loss: 0.2008 - val_acc: 0.9400\n",
      "Epoch 180/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.2494 - acc: 0.9138 - val_loss: 0.2004 - val_acc: 0.9400\n",
      "Epoch 181/200\n",
      "232/232 [==============================] - 0s 190us/step - loss: 0.2490 - acc: 0.9138 - val_loss: 0.1998 - val_acc: 0.9400\n",
      "Epoch 182/200\n",
      "232/232 [==============================] - 0s 199us/step - loss: 0.2485 - acc: 0.9138 - val_loss: 0.1993 - val_acc: 0.9400\n",
      "Epoch 183/200\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.2480 - acc: 0.9138 - val_loss: 0.1988 - val_acc: 0.9400\n",
      "Epoch 184/200\n",
      "232/232 [==============================] - 0s 216us/step - loss: 0.2476 - acc: 0.9138 - val_loss: 0.1983 - val_acc: 0.9400\n",
      "Epoch 185/200\n",
      "232/232 [==============================] - 0s 205us/step - loss: 0.2470 - acc: 0.9138 - val_loss: 0.1978 - val_acc: 0.9400\n",
      "Epoch 186/200\n",
      "232/232 [==============================] - 0s 183us/step - loss: 0.2466 - acc: 0.9138 - val_loss: 0.1973 - val_acc: 0.9400\n",
      "Epoch 187/200\n",
      "232/232 [==============================] - 0s 195us/step - loss: 0.2462 - acc: 0.9138 - val_loss: 0.1968 - val_acc: 0.9400\n",
      "Epoch 188/200\n",
      "232/232 [==============================] - 0s 168us/step - loss: 0.2457 - acc: 0.9138 - val_loss: 0.1963 - val_acc: 0.9400\n",
      "Epoch 189/200\n",
      "232/232 [==============================] - 0s 194us/step - loss: 0.2453 - acc: 0.9138 - val_loss: 0.1958 - val_acc: 0.9400\n",
      "Epoch 190/200\n",
      "232/232 [==============================] - 0s 182us/step - loss: 0.2448 - acc: 0.9138 - val_loss: 0.1953 - val_acc: 0.9400\n",
      "Epoch 191/200\n",
      "232/232 [==============================] - 0s 178us/step - loss: 0.2444 - acc: 0.9138 - val_loss: 0.1948 - val_acc: 0.9400\n",
      "Epoch 192/200\n",
      "232/232 [==============================] - 0s 200us/step - loss: 0.2440 - acc: 0.9138 - val_loss: 0.1944 - val_acc: 0.9400\n",
      "Epoch 193/200\n",
      "232/232 [==============================] - 0s 204us/step - loss: 0.2435 - acc: 0.9138 - val_loss: 0.1939 - val_acc: 0.9400\n",
      "Epoch 194/200\n",
      "232/232 [==============================] - 0s 198us/step - loss: 0.2431 - acc: 0.9138 - val_loss: 0.1934 - val_acc: 0.9400\n",
      "Epoch 195/200\n",
      "232/232 [==============================] - 0s 210us/step - loss: 0.2427 - acc: 0.9138 - val_loss: 0.1930 - val_acc: 0.9400\n",
      "Epoch 196/200\n",
      "232/232 [==============================] - 0s 196us/step - loss: 0.2422 - acc: 0.9138 - val_loss: 0.1925 - val_acc: 0.9400\n",
      "Epoch 197/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.2418 - acc: 0.9138 - val_loss: 0.1921 - val_acc: 0.9400\n",
      "Epoch 198/200\n",
      "232/232 [==============================] - 0s 193us/step - loss: 0.2414 - acc: 0.9138 - val_loss: 0.1916 - val_acc: 0.9400\n",
      "Epoch 199/200\n",
      "232/232 [==============================] - 0s 191us/step - loss: 0.2410 - acc: 0.9138 - val_loss: 0.1911 - val_acc: 0.9400\n",
      "Epoch 200/200\n",
      "232/232 [==============================] - 0s 186us/step - loss: 0.2406 - acc: 0.9138 - val_loss: 0.1907 - val_acc: 0.9400\n",
      "\n",
      "\n",
      "\n",
      "Accuracy Score for Nerual Network is 94.000\n",
      "\n",
      "Nerual Network roc-auc is 0.945\n",
      "\n",
      "Nerual Network Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97        93\n",
      "         1.0       0.67      0.57      0.62         7\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       100\n",
      "   macro avg       0.82      0.77      0.79       100\n",
      "weighted avg       0.95      0.95      0.95       100\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAE/CAYAAAA9lHapAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYLGV59/Hv7xxUQEQWBRFUBERc3oCiRjFRFkVwAyUuRNkknmyKS8QQyAJGo0GNS2KIhygiouIummhEAsQYXAAJomjQuEA4KLKo7HC43z+qBpqhZ6Zn6J7pnvp+zlXXma6qrrqrpqbvfpZ6KlWFJEnL3YqlDkCSpMVgwpMkdYIJT5LUCSY8SVInmPAkSZ1gwpMkdYIJT5LUCSa8eUryu0nOSXJtkjVJvpDkt3qWPzLJqUl+meTXSc5IskvP8q2TVJJ/mbbdDyU5uv151yS3tfuYmj7XLvtAkjdOe+/UNtdpX/9Wkv9qY7gqyVeTPL5ddnCS/5z2/oOTfDvJ9UkuT3Jcko16lh/dbv8FPfPWaedtPcN5OrNdvuO0+Z9p5+/aJ4ZK8sKeeS/pOf4bpp+Tdp0ft8uubWP/QJINerZx+/lK8pj2nGzXs3znJNfMchyV5Lqe/V7Ts2yj9lxd3p67byc5ZNr7Z41vhvN2Y7v+L5J8KskWfda7y/lq5+/azn/PtPn/meTgnveu7TmmHyU5Icn2095zryRvTvLT9hguTnJ4kkyLd+Dfc8/yDyS5edo1/qJpxzfXNXnL1O+kvd6fNO08TP8bunZqnSSPSvKlJFe37z83yTMHueY0uUx485DktcA7gb8BNgceDPwjsE+7fFvgq8C3gYcCDwQ+DXyp94+x9cQkT55ld5dV1QY903MGjHFD4PPA3wObAFsCxwA3zbD+nwB/CxwO3Bd4IvAQ4LQk9+xZ9SrgDUlWDhJH63+AA3v2tWm7/Sv6rHtQu4+DpmZU1clTxw/szbRz0vPe57SvdwIeA/xZv2Cq6lvAe4Dj07gH8H7gL6vqx7Mcx449+92oPZZ7Al+mOVdPojl3hwNvaa+TXgPF1+MV7frbARsAb+uzzl3OV4/rgANnSuKts9t93Bd4GnADcG6SR/es83FgD+CZwH2AA4BVwLumbWs+v+dex067xk9p3z/oNXlKewz3A85o4+01/W9og6o6u132OeA0mr/jzYDDgF/N45rTBDLhDSjJfYE3AH9cVZ+qquuq6paq+lxVHd6udjTNB8lRVXVVVf26qt4NnETzB9zrWOCNDN/2AFX1kapaW1U3VNWXquqCPse0IU0yfGVVfbE9nh8DL6T5gHlpz+pfBG6eNm8uJwMv6kmS+9N8Abh5WhwPAZ5K82H6jCSbz2Mft6uqy4F/o0ksMzkG2KLd15HAtcA/LGB3B9B84XlBVf2oPXdfpPngfEN7bhcSX+/61wCfmb7+AOfrGuADwF8NsI+1VfXDqvoj4Cyaa5gkewB7AvtV1YVVdWtVfY3m9//HvaVkBvw9D2Ke1+TUMdzaxrBlkvsPsI/70XwhPb6qbm6nr1bVf871Xk02E97gngSsS/OHPJOnc9dvmQAfA56cZP2eee8Btk/ytOGFCDTfttcmOTHJ3kk2nmXdXWiO6VO9M6vqWuALNMdz+2zgL4C/aktGg7gM+C7NByc0pYAP9lnvQOCcqvokcBHwkgG3fydJtqL5Vv6DmdapqpuAQ2m+gPwJcGhV3baA3T0d+EJVXTdt/idpzun0Ev1A8U1bf1Pg+X3WH+R8vQnYL8nDB9lX61PAb7c/Px34elVd0rtCVX0duJSm5Ddl0N/zIOZzTQK3l7YPBK4Erh5gH1fSnNMPJdl3oV+wNHlMeIPbFPhF+21yJvcD1vSZv4bmXPcmnxtpPpRmKuU9sG1bmJpeOMN6d1JVvwJ+iyZBHQ9ckaZNsd8f9f2Y+ZjWtMt7t30qTTXV7w0SS+uDNNVrDwc26qlS6nUg8OH25w/Tv5puNp9J8mvgEuDnzF2yuRC4Ffh2VX1vgO2f1/N7eHc7r+/vuj2Xv+DO526+8b07yS97tvPKacvnPF9tafKfaGolBnUZTTU4zHwtQ59rg8F+z9O9rue8/qJnv4Neky9M06Z6A/By4HemvW/639A1Se5dzQDCuwE/Bt4OrEnyH0keNkDMmmAmvMFdCdwvbceQGfyCprpsui2A27jrt8/jgc2T9Gufu6yqNuqZPtbOvxWYXsK6R7v92wCq6qKqOriqtgIeTdOW+M4Z4p3pmLZol0/358BRNN/CB/EpYHeaD+2Tpi9s2zEfCny0nfVh4P8lGajar7VvVd0H2BXYgbt+GE/3dprqu62SvHiA7T+25/dwWDuv7++6PZf3487nbr7xHVZV9wV+g+ZL0lY925/P+fpbmirPHfss62dLmnZBmPlahv7Xxqy/5xm8ree8Tp2T+VyTH2vbVDen+RKz87T1p/8NbTRVIq+qS6vqFVW1LU1V6XUsvFSqCWHCG9zZNKWyfWdZ58vAC/rMfyFN2971vTOr6haa9oq/BtLnff38FNh62ryHApf0q5prSzAfoEl8051N05nl+b0zk9ybpurt9D7bO42mOuiPBgm2PeYvAH9I/w/Cg2iO/fwklwNfb+cf2GfdufZ1Fs2x9uvkAdzeNrUP8Aft9K4km8y0/iy+DOzdnqte+9Gc068tJL5p63+bpgbgPcntPSMHPl9VdSXNF52/HmR/wPOAr7Q/fxn4zSQP6l0hyROABwH/Pm1fc/2eB7WQa/IXwO8DR6dPj9a5tNW276H/34iWERPegKrql8Bf0nz47Jtk/ST3aNvJjm1XOwbYJcmbkmyS5D5JXknzYfSnM2z6JOBewF4DhvJJ4FlJ9kyyMskDaUpdHwVIskOSP2nbi2g/sPan/wfwL9uY/z7JXu3xbE3TDnkpM39wHQW8fsB4oekc8tTpPSGTrEvzZWAVTceMqemVwEvmKE3P5J3A0/uVeNoPzeOBV1fVFVX1BZqeeu9YwH5OojlHH09zW8g9kjwDeDdwdHtu5xXfDE6k6UX43AWer7+jaRd7RL+Nt9fQQ5P8PU0J9BiAqvoyTXL5ZJou/CuTPJGmc8hxVXVxn831/T3Px0KvyfaL3b8xwHWZZOMkxyTZLsmKthPLy+jzN6LlxYQ3D1X1d8BraRLMFTRtMq+g6UlH+yHwW8CONO0Da2i+8T+jqr46wzbX0rTpDFTKqKrv0CSwN9NUP51N8y3/mHaVXwO/CXw9yXU0f8QX0nTQ6Le9Y2k+qN4G/Krd1iXAHm0Hj37v+SrwjUHibde/bIYecPvStL98sKoun5qA9wErGfxLQO++rqCpmvqLPov/BvheVZ3cM+/VNCW1PfusP9t+bqLpzn8JzTn7FU1yOaqq3rrA+PqtfzNNEv0LFnC+2jbdY7nr9fWkNPeV/Qo4E9gQeHxbqpyyH013/y/S9Gb9ULuv6W2KU/ua6fc8Lwu5JltvBVYl2ax9/cDc9T68/Wh6j25NU4r9Fc3fx03AwXc3do23lA+AlSR1gCU8SVInmPAkSZ1gwpMkdYIJT5LUCSY8SdJYS/KqJBcm+U6SV7fzNklyWpqneJw2xzCKzXZG3UtzvQfvbzdQTbwbfnrM3CtJE2H7QQe5mLeFfN7f8NOPzBpPmid4fBR4As0tJV+kGeDg5cBVVfWWJEcAG1fVTPc7A5bwJEnj7RHA16rq+nas1LNoRgXah2ZgBtr/ZxsFCzDhSZKGJFkx72kAFwJPSbJp+8SZZ9IMb7d5Va0BaP/fbJZtALCQoZskSbqLLKAMlWQVzXB5U1ZX1eqpF1V1UZK/pRkG8Frgv2kG0Z83E54kaSgGLLHdSZvcVs+xzvtohrUjyd/QjKv6syRbVNWadtDwn8+1L6s0JUlDMaIqTabGR03yYJonaXwEOJU7ngV5EPDZubZjCU+SNBR3PMVq6D6ZZFPgFuCPq+rqJG8BPpbkUJrHpvV7NNudmPAkSUMymkrDqvrtPvOuBPaYz3ZMeJKkoVhIG95iMuFJkobChCdJ6oSF3JawmEx4kqShsIQnSeoEE54kqRNMeJKkTggjuw9vKEx4kqShsIQnSeoEE54kqRPGPeGNd3SSJA2JJTxJ0pCMdxnKhCdJGopxr9I04UmShsKEJ0nqBMfSlCR1giU8SVInjPCJ50NhwpMkDYUlPElSJ9iGJ0nqBEt4kqROMOFJkjrBKk1JUjdYwpMkdYFVmpKkTvA+PElSJ9iGJ0nqhHGv0hzv6CRJGhJLeJKk4bANT5LUCWNeZ2jCkyQNhyU8SVInmPAkSZ1glaYkqQvKEp4kqRPGO9+Z8CRJQ7JivDOeCU+SNBxWaUqSOmEE+S7Jw4FTemZtA/wlsBHwcuCKdv6RVfWvs23LhCdJGo4RVGlW1feBnQCSrAT+D/g0cAjwjqp626DbMuFJkoZj9FWaewA/rKqfLORRRGN+14QkaWJkAdP8vBj4SM/rVyS5IMn7k2w815tNeJKk4ViReU9JViU5p2da1W/TSe4JPBf4eDvrOGBbmurONcDb5wrPKk1J0nAsoEazqlYDqwdYdW/gvKr6Wfu+n92+2+R44PNzbcCEJ0kaihGPtLI/PdWZSbaoqjXty+cBF861AROeJGmsJVkfeDrw+z2zj02yE1DAj6ct68uEJ0kajhGNtFJV1wObTpt3wHy3Y8KTJA3HeA+0YsKTJA2JQ4tJkjrBwaMlSZ0w3vnOhCdJGhKrNCVJnWDCkyR1wpgPVmnCkyQNhyU8SVInjHe+G/cCqPr545ftxTmnHcu5X34rrzh0bwCe/6zf5Nwvv5Xrfnwyj/2NbZY4Qml+1qy5ggMOOJK99/5DnvWsP+LEE09d6pC0ALUi854WkyW8CfPI7bfikP1357ef8+fcfMutnHrSEXzh9G/xne9fwotX/R3/8ObfW+oQpXlbuXIlRxzxMh71qO249trr2W+/1/DkJ+/Edts9eKlD03yMeZWmJbwJs8PDtuQb513MDTfezNq1t/GVr13EPns9nu//4DIu/t81c29AGkObbbYJj3rUdgBssMH6bLPNg/jZz65c4qg0b6N/AOzdMmsJL8l9gb2ALWlGpL4M+LequmYRYlMf3/n+JRx9+IvYZKMNuOHGm9lrt50474IfLXVY0tBceunPuOiiH7Ljjg9f6lA0X2M+0sqMJbwkBwLnAbsC6wP3BnYDzm2XaQl8/weX8fbjTuXzJx/JqScdwQUX/ZRb165d6rCkobjuuhs47LA3c+SRL2eDDdZf6nA0X8n8p0U0WwnvKGDn6aW5JBsDXwc+ONMb20e0rwJYZ+PHsc4G2w0hVE058ZQzOfGUMwE45vUv4v/WXLW0AUlDcMstt3LYYW/mOc/ZlT333GWpw9FCjHcBb9Y2vNBUY053G3McVlWtrqrHVdXjTHbDd/9NNwTgQQ/clH32ejwfO/W/ljgi6e6pKo466t1ss82DOOSQfZc6HC1Ts5Xw3gScl+RLwCXtvAfTPHX2r0cdmGb2kfe+hk023oBbblnLq//iBK755XU89xmP4+/ecDD322RDPnXC67nguz/muQe8ZalDlQZy7rnf5bOfPYPtt9+affY5DIDXvvZAnvrUxy1xZJqXMW/DS1W/Qly7sKm+fAZNp5UAl9J0Wrl60B2s9+D9Z96BNCFu+OkxSx2CNCTbjywrbXvox+f9ef/D971g0bLkrL0028T20UWKRZI0wWq8C3iD3YeXZPVsryVJYkXmPy2iQUdaee8cryVJXTfmI60MlPCq6tzZXkuSNO6dVmZMeEk+R//bEgCoqueOJCJJ0mQa88EqZyvhvW3RopAkTb5JrdKsqrMWMxBJ0oSb1CrNKUkeBrwZeCSw7tT8qvKha5Kk29WYl/AGqXE9ATgOuJVm8OgPAieNMihJ0gRasYBpkcOby3pVdTrNqCw/qaqjgd1HG5YkaeIsg/vwbkyyArg4ySuA/wM2G21YkqSJswyqNF9N8zy8w4CdgQOAg0YZlCRpAk16Ca+qvtn+eC1wyGjDkSRNrPEu4A3US/MM+tyAXlW240mSbleTflsC8Lqen9cF9qPpsSlJ0h0mPeH1GTfzq0m8KV2SNFEGqdLcpOflCpqOKw8YWUSSpMk05r00B6nSPJemDS80VZk/Ag4dZVCSpAk0wYNHT3lEVd3YOyPJvUYUjyRpUo15CW+QfPxffeadPexAJEkTbkT34SXZKMknknwvyUVJnpRkkySnJbm4/X/jOcObZQcPSLIzsF6SxyR5bDvtSnMjuiRJdxjdjefvAr5YVTsAOwIXAUcAp1fVw4DT29ezmq1K8xnAwcBWwNu545bCXwFHDhqlJKkbRvG0hCQbAk+hyUdU1c3AzUn2AXZtVzsROBP409m2Ndvz8E4ETkyyX1V98m5HLUla3kbTaWUb4ArghCQ70nSkfBWweVWtAaiqNUnmHON5kPB2TrLR1IskGyd548LiliQtW8m8pySrkpzTM62attV1gMcCx1XVY4DrGKD6sp9BEt7eVXXN1Iuquhp45kJ2JklaxhbQhldVq6vqcT3T6mlbvRS4tKq+3r7+BE0C/FmSLQDa/38+Z3gDHMLK3tsQkqwHeFuCJOnORtBppaouBy5J8vB21h7Ad4FTuePJPQcBn51rW4Pch/ch4PQkJ7SvD6FpIJQk6Q6juw3vlcDJSe4J/C9NHloBfCzJocBPgRfMtZFBxtI8NskFwNNoDueLwEPuRuCSpGVoVE9LqKrzgcf1WbTHfLYzSAkP4HLgNuCFNEOL2WtTknRnYz7SyowJL8n2wIuB/YErgVOAVNVuixSbJGmSTPDjgb4HfAV4TlX9ACDJaxYlKknS5BnvfDdrL839aKoyz0hyfJI9GPvDkSQtlRUr5j8tanwzLaiqT1fVi4AdaIZseQ2weZLjkuy5SPFJkjQUc+bXqrquqk6uqmfTjKt5Pgu8y12StHwtYKCVRTWvAmVVXVVV762q3UcVkCRpMo17whv0tgRJkmaVSb0tQZKk+RjzfGfCkyQNhwlPktQJWeTbDObLhCdJGgpLeJKkThjzkcVMeJKk4bCEJ0nqBBOeJKkTvA9PktQJ9tKUJHXCmBfwTHiSpOEw4UmSOsGEJ0nqhHG/D2/MmxglSRoOS3iSpKGwSlOS1AkmPElSJ2TMG/FMeJKkobCEJ0nqBBOeJKkTTHiSpE4Y8yY8E54kaTgs4UmSOsGnJUiSOsESniSpE3wArCSpE8Y835nwJEnD0fmEd/1P/3LUu5BGbm3dvNQhSEOxcoRJqfMJT5LUDeN+H96YdyKVJE2KFZn/NKgkK5N8K8nn29cfSPKjJOe3005zbcMSniRpErwKuAjYsGfe4VX1iUE3YAlPkjQUK1LzngaRZCvgWcA/36347s6bJUmaspAqzSSrkpzTM63qs+l3Aq8Hbps2/01JLkjyjiT3mjO+YRykJEkrFjBV1eqqelzPtLp3m0meDfy8qs6dtrs/A3YAHg9sAvzpXPHZhidJGopBqyjn6cnAc5M8E1gX2DDJh6rqpe3ym5KcALxuzvhGEZ0kqXtG0Uuzqv6sqraqqq2BFwP/XlUvTbIFQJrxzPYFLpxrW5bwJElDscglqJOT3B8IcD7wB3O9wYQnSRqKUd94XlVnAme2P+8+3/eb8CRJQ5HRtOENjQlPkjQU4z60mAlPkjQU494L0oQnSRqKEd2WMDQmPEnSUFilKUnqBKs0JUmdYAlPktQJtuFJkjph3Et4417lKknSUFjCkyQNxbiXoEx4kqShsA1PktQJ496GZ8KTJA2FCU+S1Am24UmSOsE2PElSJ1ilKUnqBKs0JUmdYAlPktQJsQ1PktQFlvAkSZ1gG54kqRO8LUGS1AlWaUqSOsGEJ0nqhJVLHcAcTHiSpKEY9za8ce9UI0nSUFjCkyQNhW14kqROMOFJkjphpQlPktQFlvAkSZ0w7r00TXiSpKGwhCdJ6gRvPJckdcK4l/C88VySNBQrUvOe5pJk3STfSPLfSb6T5Jh2/kOTfD3JxUlOSXLPOeMbwjFKksTKzH8awE3A7lW1I7ATsFeSJwJ/C7yjqh4GXA0cOteGTHiSpKFYkflPc6nGte3Le7RTAbsDn2jnnwjsO2d8CzoqSZKmGUXCA0iyMsn5wM+B04AfAtdU1a3tKpcCW84Z38IOS5KkO1tIwkuyKsk5PdOq6dutqrVVtROwFfAE4BF9dj9ng6C9NCVJQ7FyATeeV9VqYPWA616T5EzgicBGSdZpS3lbAZfN9X5LeJKkoVixgGkuSe6fZKP25/WApwEXAWcAv9OudhDw2bm2ZQlPkjQUI7oPbwvgxCQraXLkx6rq80m+C3w0yRuBbwHvm2tDJjxJ0tiqqguAx/SZ/7807XkDM+FJkoZi3EdaMeFJkoZiIZ1WFpMJT5I0FJbwJEmdYMKTJHWCCU+S1AkDDga9ZEx4kqShGORxP0vJhCdJGopxH7rLhDfBbrrpZl76kqO4+eZbWLt2LXs+YxcOO2z/pQ5LWpC1a9fygt/5UzbfbBOOe++RSx2OFsA2PI3MPe95Dz5w4hu4973X45ZbbuUlv/tnPOUpj2WnnR6+1KFJ83bSB/+FbbfZkmuvvWGpQ9ECjXsb3riXQDWLJNz73usBcOuta7n11rUkY37FSX1cfvmVnHXWeez3gqctdSi6G1ak5j0tanwLeVOSpw87EC3M2rVr2XefV/PkXQ5il112ZMcdt1/qkKR5e8vfvJ/Xve4AVviFbaKN6gGwQ4tvge+bc1RqLY6VK1fymc++kzPP+mcuuOBi/ud/frLUIUnzcuYZ57DJpvflUY/edqlD0d007glvxja8JKfOtAjYdLaNtk+sXQXwT+89mlWrXrjgADWYDTfcgCf85qP5yle+xfbbP2Spw5EGdt553+OMf/8m/3HWedx08y1cd+31vP7wd3HsW1+11KFpnsa9jSxV/etQk1wNvBS4dvoi4JSq2nyQHRQXjfeNGRPsqqt+yTrrrGTDDTfgxhtv4tCXHc3vvfz57Lbb45c6tGXntlq71CF0wje+fiEnvP9Ue2mO0Mo8emTlqm9c8S/z/rx/wv2ftWjlvNl6aX4NuL6qzpq+IMn3RxeSBnXFz6/miCPexdq1t1FV7LXXk012kpbMuLfAzljCGxZLeFoOLOFpuRhlCe+bCyjhPX5MSniSJA1s3DvZDtTGmGT1bK8lSVqxgGkxDVrCe+8cryVJHZflMHh0VZ0722tJksa8RnPW+/A+B8yYrqvquSOJSJI0kca9DW+2Et7bFi0KSdLEG/N8N3PC63f/nSRJM5n4xwMleRjwZuCRwLpT86tqmxHGJUmaMGOe7wbqFXoCcBxwK7Ab8EHgpFEGJUmaPMn8p8U0SMJbr6pOpxmV5SdVdTSw+2jDkiRNmixgWkyD3JZwY5IVwMVJXgH8H7DZaMOSJE2a5VCl+WpgfeAwYGfgAOCgUQYlSZo8E/s8vClV9c32x2uBQ0YbjiRpUo17CW+QXppn0OcG9KqyHU+SdLvlMLTY63p+XhfYj6bHpiRJt5v4El6fcTO/msSb0iVJdzLJQ4sBkGSTnpcraDquPGBkEUmSNAKDVGmeS9OGF5qqzB8Bh44yKEnS5Fns59vN1yAJ7xFVdWPvjCT3GlE8kqQJNe5VmoMk5P/qM+/sYQciSZpsEzvSSpIHAFsC6yV5DHfEtiHNjeiSJN1uFCW8JO8Hng38vKoe3c47Gng5cEW72pFV9a9zbWu2Ks1nAAcDWwFv546E9yvgyIUELklavkZUYvsA8A80Dy7o9Y6qmtdzW2d7Ht6JwIlJ9quqT847RElSp4xiqLCq+o8kWw9jW4O04e2cZKOpF0k2TvLGYexckrR8LHIb3iuSXJDk/Uk2HuQNgyS8vavqmqkXVXU18MyFRihJWp6SWsCUVUnO6ZlWDbCr44BtgZ2ANTTNbnMa5LaElUnuVVU3NQeU9QBvS5Ak3clCSmxVtRpYPc/3/Oz2fSbHA58f5H2DJLwPAacnOaF9fQhw4nyCkyQtf4t1H16SLapqTfvyecCFg7xvkLE0j01yAfA0mgT+ReAhCw1UkrQ8jSLfJfkIsCtwvySXAn8F7JpkJ5pRwH4M/P4g2xqkhAdwOXAb8EKaocXstSlJupNRDC1WVfv3mf2+hWxrthvPtwdeDOwPXAmcAqSqdlvIjiRJy9u4Dy02Wwnve8BXgOdU1Q8AkrxmUaKSJE2g8c54s5VA96OpyjwjyfFJ9mDcj0aStGSygH+LacaEV1WfrqoXATsAZwKvATZPclySPRcpPknShEhWzHtaTHPuraquq6qTq+rZNONqng8cMfLIJEkaonml16q6qqreW1W7jyogSdKkGu8HBA16W4IkSbNa7Da5+TLhSZKGxIQnSeqAxe6EMl8mPEnSkFjCkyR1gG14kqROMOFJkjrCNjxJUgdkzEePNuFJkobEhCdJ6gDb8CRJHWEbniSpAyzhSZI6wU4rkqSOMOFJkjogtuFJkrphvEt4452OJUkaEkt4kqShsNOKJKkjTHiSpA6w04okqSMs4UmSOsCRViRJnWCnFUlSR9iGJ0nqAKs0JUkdYcKTJHWAbXiSpI6wDU+S1AHj3oaXqlrqGHQ3JVlVVauXOg7p7vJa1iiNd/lTg1q11AFIQ+K1rJEx4UmSOsGEJ0nqBBPe8mCbh5YLr2WNjJ1WJEmdYAlPktQJJrwRSbI2yflJLkzy8STr341t7Zrk8+3Pz01yxCzrbpTkjxawj6OTvK7P/CR5d5IfJLkgyWPnu21NtmV0Le+Q5OwkN/VbruXPhDc6N1TVTlX1aOBm4A96F7aJZN7nv6pOraq3zLLKRsC8PyRmsTfwsHZaBRw3xG1rMiyXa/kq4DDgbUPcpiaICW9xfAXYLsnWSS5K8o/AecCDkuzZfus8r/32vAFAkr2SfC/JfwLPn9pQkoOT/EP78+ZJPp3kv9tpF+AtwLbtN/K3tusdnuSbbQntmJ5tHZXk+0m+DDx8htj3AT5Yja8BGyXZYvinSBNiYq/lqvp5VX0TuGU0p0bjzoQ3YknWoSklfbud9XCaBPIY4Drgz4GnVdVjgXOA1yZZFzgeeA7w28ADZtj8u4GzqmpH4LHAd4AjgB+238gPT7InTensCcBOwM5JnpJkZ+BhEzKFAAABvklEQVTFwGNoPoQe3xPzHySZ+ha/JXBJzz4vbeepY5bBtayOcyzN0Vkvyfntz18B3gc8EPhJW1ICeCLwSOCr7Sjj9wTOBnYAflRVFwMk+RD9R6DYHTgQoKrWAr9MsvG0dfZsp2+1rzeg+dC4D/Dpqrq+3cepU2+oqn/qeX+/wfHs2tsty+VaVseZ8EbnhqraqXdG+0FwXe8s4LSq2n/aejsxvKQS4M1V9d5p+3j1gPu4FHhQz+utgMuGFJsmw3K5ltVxVmkura8BT06yHUCS9ZNsD3wPeGiSbdv19p/h/acDf9i+d2WSDYFf03zjnfJvwMt62lO2TLIZ8B/A85Ksl+Q+NFVO/ZwKHNh2THgi8MuqWrPQA9ayNQnXsjrOhLeEquoK4GDgI0kuoPnQ2KGqbqSp9vmXtqH/JzNs4lXAbkm+DZwLPKqqrqSpVrowyVur6kvAh4Gz2/U+Adynqs4DTgHOBz5JU1UF3KXd41+B/wV+QNMWM8xec1omJuFaTvKAJJcCrwX+PMmlbWJVRzjSiiSpEyzhSZI6wYQnSeoEE54kqRNMeJKkTjDhSZI6wYQnSeoEE54kqRNMeJKkTvj/7VuTPQ+tGCsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\jamie\\\\AppData\\\\Desktop\\\\PhD\\\\Datasets\\\\MSc-Sleep-Data\\\\AllSubjects.csv')\n",
    "\n",
    "\n",
    "df.head(33)\n",
    "\n",
    "df = df.drop('Date', 1)\n",
    "df = df.drop('Gender', 1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#RANDOM FOREST\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "\n",
    "#y-axis uses the variable outcome as its target variable\n",
    "y = df[\"Outcome\"].values\n",
    "\n",
    "#splitt the training data using 70% for training and 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11111)\n",
    "\n",
    "#shows percentage of seizures compared to non seizures\n",
    "np.mean(y), np.mean(1-y)\n",
    "\n",
    "\n",
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "#list the performance metrics the model emanates\n",
    "print('\\nAccuracy Score for Random Forest Model is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)*100))\n",
    "print('\\nRandom Forest roc-auc measure is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))\n",
    "print(\"\\nRandom Forest  Classification Report\\n\")\n",
    "print(classification_report(y_test,y_pred_class_rf))\n",
    "\n",
    "\n",
    "\n",
    "#create a confusion matrix to visually display the postive and negative prediction rate\n",
    "cmrf=confusion_matrix(y_test,y_pred_class_rf)\n",
    "conf_matrixrf=pd.DataFrame(data=cmrf,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.title(\"CONFUSION MATRIX FOR RANDOM FOREST\")\n",
    "sns.heatmap(conf_matrixrf, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.savefig('cmrf.png')\n",
    "\n",
    "\n",
    "#NEURAL NETWORK\n",
    "\n",
    "## normalize the data\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(5,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "print(\"\\n************Start of Nerual Netwrok Summary*********\")\n",
    "model_1.summary()\n",
    "\n",
    "# Fit(Train) the Model\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)\n",
    "\n",
    "y_pred_class_nn_1[:10]\n",
    "y_pred_prob_nn_1[:10]\n",
    "\n",
    "# Print model performance and plot the roc curve\n",
    "print('\\n')\n",
    "print('\\nAccuracy Score for Nerual Network is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)*100))\n",
    "print('\\nNerual Network roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "print(\"\\nNerual Network Classification Report\\n\")\n",
    "print(classification_report(y_test,y_pred_class_rf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
