{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn as sk\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#KERAS\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 226\n",
      "Class 1: 22\n",
      "Proportion: 10.2727 : 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\jamie\\\\AppData\\\\Desktop\\\\PhD\\\\Datasets\\\\MSc-Sleep-Data\\\\Book4.csv')\n",
    "\n",
    "df = df.drop('Date', 1)\n",
    "df = df.drop('Gender', 1)\n",
    "\n",
    "target_count = df.Outcome.value_counts()\n",
    "\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 4), ': 1')\n",
    "\n",
    "target_count.plot(kind='bar', title='Count (Outcome)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>Exercise</th>\n",
       "      <th>Stress</th>\n",
       "      <th>Fatigued</th>\n",
       "      <th>Sleep</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5  Exercise  Stress  Fatigued  Sleep  Outcome\n",
       "0  2         0     4.0       5.0   13.0        1\n",
       "1  2         0     3.0       5.0   13.0        0\n",
       "2  2         0     3.0       5.0   13.0        0\n",
       "3  2         0     3.0       5.0   13.0        0\n",
       "4  2         1     4.0       4.0   10.0        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df.Outcome.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df[df['Outcome'] == 0]\n",
    "df_class_1 = df[df['Outcome'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "1    22\n",
      "0    22\n",
      "Name: Outcome, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.Outcome.value_counts())\n",
    "\n",
    "df_test_under.Outcome.value_counts().plot(kind='bar', title='Count (Outcome)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    226\n",
      "0    226\n",
      "Name: Outcome, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over.Outcome.value_counts())\n",
    "\n",
    "df_test_over.Outcome.value_counts().plot(kind='bar', title='Count (Outcome)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>Exercise</th>\n",
       "      <th>Stress</th>\n",
       "      <th>Fatigued</th>\n",
       "      <th>Sleep</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    5  Exercise  Stress  Fatigued  Sleep  Outcome\n",
       "1   2         0     3.0       5.0   13.0        0\n",
       "2   2         0     3.0       5.0   13.0        0\n",
       "3   2         0     3.0       5.0   13.0        0\n",
       "20  2         1     3.5       4.0    9.5        0\n",
       "21  2         0     3.0       4.0    7.0        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_over.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8495575221238938\n",
      "[[47 10]\n",
      " [ 7 49]]\n",
      "Accuracy score for SVM is: 84.95575221238938\n",
      "\n",
      "SVM Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85        57\n",
      "           1       0.83      0.88      0.85        56\n",
      "\n",
      "    accuracy                           0.85       113\n",
      "   macro avg       0.85      0.85      0.85       113\n",
      "weighted avg       0.85      0.85      0.85       113\n",
      "\n",
      "\n",
      "SVM AUCROC is 0.850\n",
      "Confusion Matrix for LOGISTICAL REGRESSION: \n",
      " [[47 10]\n",
      " [ 7 49]]\n",
      "Accuracy :  0.8495575221238938\n",
      "Sensitivity :  0.8245614035087719\n",
      "Specificity :  0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test,y_train,y_test = train_test_split(df_test_over.drop('Outcome',axis=1),df_test_over['Outcome'],\n",
    "                                                  test_size=0.25,random_state=111)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "print(np.mean(predicted == y_test))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "\n",
    "\n",
    "#Evaluation of performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Accuracy score for SVM is:\",accuracy_score(y_test, predicted)*100)\n",
    "print(\"\\nSVM Classification Report\\n\")\n",
    "print(classification_report(y_test,predicted))\n",
    "print('\\nSVM AUCROC is {:.3f}'.format(roc_auc_score(y_test,predicted)))\n",
    "\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test, predicted)\n",
    "print('Confusion Matrix for LOGISTICAL REGRESSION: \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Logistical Regression Model is: 92.64705882352942\n",
      "\n",
      "Logisitcal Regression Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        66\n",
      "           1       0.88      1.00      0.93        70\n",
      "\n",
      "    accuracy                           0.93       136\n",
      "   macro avg       0.94      0.92      0.93       136\n",
      "weighted avg       0.94      0.93      0.93       136\n",
      "\n",
      "\n",
      "Logisitcal Regression AUCROC is 0.924\n"
     ]
    }
   ],
   "source": [
    "#LOGISTICAL REGRESSION\n",
    "#Building a Logistic Regression model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the epileptic data: 70%training||30% testing using the SKLearn train_test_Split\n",
    "x_train, x_test,y_train,y_test = train_test_split(df_test_over.drop('Outcome',axis=1),df_test_over['Outcome'],\n",
    "                                                  test_size=0.30,random_state=2)\n",
    "\n",
    "#Training and Predicting\n",
    "#Import the Logisitcal Regression Library from SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#create instance of logistical regression model called logmodel\n",
    "logmodel=LogisticRegression()\n",
    "#pass the data to the logmodel\n",
    "logmodel.fit(x_train,y_train)\n",
    "\n",
    "#make predictions with model on unseen test data\n",
    "predictions = logmodel.predict(x_test)\n",
    "\n",
    "\n",
    "#Evaluation of performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Accuracy score for Logistical Regression Model is:\",accuracy_score(y_test, predictions)*100)\n",
    "print(\"\\nLogisitcal Regression Classification Report\\n\")\n",
    "print(classification_report(y_test,predictions))\n",
    "print('\\nLogisitcal Regression AUCROC is {:.3f}'.format(roc_auc_score(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[56 10]\n",
      " [ 0 70]]\n",
      "Accuracy :  0.9264705882352942\n",
      "Sensitivity :  0.8484848484848485\n",
      "Specificity :  1.0\n",
      "Precision: 0.875000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3b7fbb153048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mfalse_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_positive_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auc' is not defined"
     ]
    }
   ],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "\n",
    "cm=confusion_matrix(y_test, predictions)\n",
    "print('Confusion Matrix : \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "precision = precision_score(y_test, predictions)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic for LOGISTICAL REGRESSION')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.grid(True)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rates')\n",
    "plt.savefig('nnroc.png')\n",
    "\n",
    "\n",
    "print('\\nLogisitcal Regression AUCROC is {:.3f}'.format(roc_auc_score(y_test,predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score for Desicion Tree Model using Gini Index is  88.97058823529412\n",
      "\n",
      "Desicion Tree  Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.84      0.88        64\n",
      "         1.0       0.87      0.93      0.90        72\n",
      "\n",
      "    accuracy                           0.89       136\n",
      "   macro avg       0.89      0.89      0.89       136\n",
      "weighted avg       0.89      0.89      0.89       136\n",
      "\n",
      "Confusion Matrix for DT: \n",
      " [[54 10]\n",
      " [ 5 67]]\n",
      "Accuracy :  0.8897058823529411\n",
      "Sensitivity :  0.84375\n",
      "Specificity :  0.9305555555555556\n",
      "Precision: 0.870130\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREE WITH GINI INDEX\n",
    "\n",
    "#first 3 attributes are used for the x-axis\n",
    "X = df_test_over.values[:, 0:4]\n",
    "#attributes after number 3 are used for the y-axis\n",
    "Y = df_test_over.values[:,5]\n",
    "\n",
    "#import Accuracy Score to guage performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Split the dataset using 70% for training and 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 333)\n",
    "\n",
    "\n",
    "#implement a depth of 5, with a minimum of 5 leaf nodes and a random state of 100\n",
    "dt_clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state =333,\n",
    "                               max_depth=5, min_samples_leaf=15)\n",
    "\n",
    "#pass the training data to the decision tree\n",
    "dt_clf_gini.fit(X_train, y_train)\n",
    "\n",
    "#make prediction using unseen test data\n",
    "y_pred_gini = dt_clf_gini.predict(X_test)\n",
    "\n",
    "#methods used to show the performance metrics of the model\n",
    "print (\"\\nAccuracy Score for Desicion Tree Model using Gini Index is \", accuracy_score(y_test,y_pred_gini)*100 )\n",
    "print(\"\\nDesicion Tree  Classification Report\\n\")\n",
    "print(classification_report(y_test,y_pred_gini))\n",
    "\n",
    "\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test, y_pred_gini)\n",
    "print('Confusion Matrix for DT: \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_gini)\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d497b0b99742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfalse_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred_gini\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_positive_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_positive_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mroc_auc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auc' is not defined"
     ]
    }
   ],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_gini)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic for LOGISTICAL REGRESSION')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.grid(True)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rates')\n",
    "plt.savefig('nnroc.png')\n",
    "\n",
    "\n",
    "print('\\nLogisitcal Regression AUCROC is {:.3f}'.format(roc_auc_score(y_test,y_pred_gini)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Score for Random Forest Model is 85.294\n",
      "\n",
      "Random Forest roc-auc measure is 0.944\n",
      "\n",
      "Random Forest  Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82        56\n",
      "           1       0.88      0.88      0.88        80\n",
      "\n",
      "    accuracy                           0.85       136\n",
      "   macro avg       0.85      0.85      0.85       136\n",
      "weighted avg       0.85      0.85      0.85       136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST\n",
    "\n",
    "X = df_test_over.iloc[:, :-3].values\n",
    "\n",
    "#y-axis uses the variable outcome as its target variable\n",
    "y = df_test_over[\"Outcome\"].values\n",
    "\n",
    "#splitt the training data using 70% for training and 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=33)\n",
    "\n",
    "#shows percentage of seizures compared to non seizures\n",
    "np.mean(y), np.mean(1-y)\n",
    "\n",
    "\n",
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "#list the performance metrics the model emanates\n",
    "print('\\nAccuracy Score for Random Forest Model is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)*100))\n",
    "print('\\nRandom Forest roc-auc measure is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))\n",
    "print(\"\\nRandom Forest  Classification Report\\n\")\n",
    "print(classification_report(y_test,y_pred_class_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for rf: \n",
      " [[46 10]\n",
      " [10 70]]\n",
      "Accuracy :  0.8529411764705882\n",
      "Sensitivity :  0.8214285714285714\n",
      "Specificity :  0.875\n",
      "Precision: 0.875000\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test, y_pred_class_rf)\n",
    "print('Confusion Matrix for rf: \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_class_rf)\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 09:43:31.809188  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0812 09:43:31.828167  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0812 09:43:31.833154  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0812 09:43:31.873015  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0812 09:43:31.879034  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0812 09:43:31.883021  7276 deprecation.py:323] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0812 09:43:32.020587  7276 deprecation_wrapper.py:119] From C:\\Users\\jamie\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************Start of Nerual Netwrok Summary*********\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 73\n",
      "Trainable params: 73\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 361 samples, validate on 91 samples\n",
      "Epoch 1/500\n",
      "361/361 [==============================] - 0s 711us/step - loss: 0.7713 - acc: 0.4017 - val_loss: 0.7529 - val_acc: 0.4615\n",
      "Epoch 2/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7627 - acc: 0.4017 - val_loss: 0.7442 - val_acc: 0.4615\n",
      "Epoch 3/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.7543 - acc: 0.4017 - val_loss: 0.7358 - val_acc: 0.4615\n",
      "Epoch 4/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7462 - acc: 0.4127 - val_loss: 0.7275 - val_acc: 0.4615\n",
      "Epoch 5/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7382 - acc: 0.4183 - val_loss: 0.7195 - val_acc: 0.4615\n",
      "Epoch 6/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7303 - acc: 0.4183 - val_loss: 0.7116 - val_acc: 0.4725\n",
      "Epoch 7/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7226 - acc: 0.4349 - val_loss: 0.7040 - val_acc: 0.4725\n",
      "Epoch 8/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.7152 - acc: 0.4875 - val_loss: 0.6963 - val_acc: 0.5275\n",
      "Epoch 9/500\n",
      "361/361 [==============================] - 0s 23us/step - loss: 0.7078 - acc: 0.5125 - val_loss: 0.6890 - val_acc: 0.5275\n",
      "Epoch 10/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.7006 - acc: 0.5125 - val_loss: 0.6818 - val_acc: 0.5275\n",
      "Epoch 11/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6936 - acc: 0.5125 - val_loss: 0.6748 - val_acc: 0.5275\n",
      "Epoch 12/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6867 - acc: 0.5152 - val_loss: 0.6678 - val_acc: 0.5275\n",
      "Epoch 13/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.6800 - acc: 0.5152 - val_loss: 0.6612 - val_acc: 0.5275\n",
      "Epoch 14/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6734 - acc: 0.5402 - val_loss: 0.6546 - val_acc: 0.7802\n",
      "Epoch 15/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6670 - acc: 0.7895 - val_loss: 0.6481 - val_acc: 0.8242\n",
      "Epoch 16/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.6607 - acc: 0.8172 - val_loss: 0.6419 - val_acc: 0.8242\n",
      "Epoch 17/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6546 - acc: 0.8172 - val_loss: 0.6357 - val_acc: 0.8242\n",
      "Epoch 18/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.6486 - acc: 0.8172 - val_loss: 0.6298 - val_acc: 0.8242\n",
      "Epoch 19/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.6428 - acc: 0.8172 - val_loss: 0.6237 - val_acc: 0.8242\n",
      "Epoch 20/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6367 - acc: 0.8172 - val_loss: 0.6178 - val_acc: 0.8242\n",
      "Epoch 21/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.6309 - acc: 0.8172 - val_loss: 0.6119 - val_acc: 0.8242\n",
      "Epoch 22/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6252 - acc: 0.8172 - val_loss: 0.6065 - val_acc: 0.8242\n",
      "Epoch 23/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6197 - acc: 0.8172 - val_loss: 0.6010 - val_acc: 0.8242\n",
      "Epoch 24/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.6143 - acc: 0.8172 - val_loss: 0.5957 - val_acc: 0.8242\n",
      "Epoch 25/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.6091 - acc: 0.8172 - val_loss: 0.5903 - val_acc: 0.8242\n",
      "Epoch 26/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.6038 - acc: 0.8172 - val_loss: 0.5851 - val_acc: 0.8242\n",
      "Epoch 27/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5986 - acc: 0.8338 - val_loss: 0.5801 - val_acc: 0.8242\n",
      "Epoch 28/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5936 - acc: 0.8338 - val_loss: 0.5753 - val_acc: 0.8242\n",
      "Epoch 29/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5889 - acc: 0.8338 - val_loss: 0.5705 - val_acc: 0.8242\n",
      "Epoch 30/500\n",
      "361/361 [==============================] - 0s 36us/step - loss: 0.5841 - acc: 0.8338 - val_loss: 0.5658 - val_acc: 0.8242\n",
      "Epoch 31/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5794 - acc: 0.8338 - val_loss: 0.5612 - val_acc: 0.8242\n",
      "Epoch 32/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5749 - acc: 0.8338 - val_loss: 0.5566 - val_acc: 0.8242\n",
      "Epoch 33/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5705 - acc: 0.8449 - val_loss: 0.5524 - val_acc: 0.8462\n",
      "Epoch 34/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5663 - acc: 0.8504 - val_loss: 0.5484 - val_acc: 0.8462\n",
      "Epoch 35/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5622 - acc: 0.8560 - val_loss: 0.5443 - val_acc: 0.8791\n",
      "Epoch 36/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.5582 - acc: 0.8753 - val_loss: 0.5404 - val_acc: 0.8791\n",
      "Epoch 37/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5542 - acc: 0.8753 - val_loss: 0.5366 - val_acc: 0.8791\n",
      "Epoch 38/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5504 - acc: 0.8753 - val_loss: 0.5327 - val_acc: 0.8791\n",
      "Epoch 39/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.5465 - acc: 0.8753 - val_loss: 0.5289 - val_acc: 0.8791\n",
      "Epoch 40/500\n",
      "361/361 [==============================] - 0s 33us/step - loss: 0.5426 - acc: 0.8753 - val_loss: 0.5252 - val_acc: 0.8791\n",
      "Epoch 41/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5389 - acc: 0.8753 - val_loss: 0.5215 - val_acc: 0.8791\n",
      "Epoch 42/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5351 - acc: 0.8753 - val_loss: 0.5179 - val_acc: 0.8791\n",
      "Epoch 43/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5315 - acc: 0.8753 - val_loss: 0.5144 - val_acc: 0.8791\n",
      "Epoch 44/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5279 - acc: 0.8753 - val_loss: 0.5108 - val_acc: 0.8791\n",
      "Epoch 45/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5243 - acc: 0.8753 - val_loss: 0.5074 - val_acc: 0.8791\n",
      "Epoch 46/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5208 - acc: 0.8753 - val_loss: 0.5039 - val_acc: 0.8791\n",
      "Epoch 47/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5173 - acc: 0.8753 - val_loss: 0.5006 - val_acc: 0.8791\n",
      "Epoch 48/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5140 - acc: 0.8753 - val_loss: 0.4973 - val_acc: 0.8791\n",
      "Epoch 49/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5107 - acc: 0.8753 - val_loss: 0.4941 - val_acc: 0.8791\n",
      "Epoch 50/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5074 - acc: 0.8753 - val_loss: 0.4909 - val_acc: 0.8791\n",
      "Epoch 51/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.5042 - acc: 0.8753 - val_loss: 0.4879 - val_acc: 0.8791\n",
      "Epoch 52/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.5010 - acc: 0.8753 - val_loss: 0.4847 - val_acc: 0.8791\n",
      "Epoch 53/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4979 - acc: 0.8753 - val_loss: 0.4817 - val_acc: 0.8791\n",
      "Epoch 54/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4948 - acc: 0.8753 - val_loss: 0.4787 - val_acc: 0.8791\n",
      "Epoch 55/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.4918 - acc: 0.8753 - val_loss: 0.4760 - val_acc: 0.8791\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 28us/step - loss: 0.4890 - acc: 0.8753 - val_loss: 0.4731 - val_acc: 0.8791\n",
      "Epoch 57/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4860 - acc: 0.8753 - val_loss: 0.4703 - val_acc: 0.8791\n",
      "Epoch 58/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.4831 - acc: 0.8753 - val_loss: 0.4675 - val_acc: 0.8791\n",
      "Epoch 59/500\n",
      "361/361 [==============================] - 0s 33us/step - loss: 0.4803 - acc: 0.8753 - val_loss: 0.4647 - val_acc: 0.8791\n",
      "Epoch 60/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.4774 - acc: 0.8753 - val_loss: 0.4620 - val_acc: 0.8791\n",
      "Epoch 61/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4746 - acc: 0.8753 - val_loss: 0.4593 - val_acc: 0.8791\n",
      "Epoch 62/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4719 - acc: 0.8753 - val_loss: 0.4567 - val_acc: 0.8791\n",
      "Epoch 63/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4692 - acc: 0.8753 - val_loss: 0.4541 - val_acc: 0.8791\n",
      "Epoch 64/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4665 - acc: 0.8753 - val_loss: 0.4515 - val_acc: 0.8791\n",
      "Epoch 65/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4639 - acc: 0.8753 - val_loss: 0.4491 - val_acc: 0.8791\n",
      "Epoch 66/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4613 - acc: 0.8753 - val_loss: 0.4466 - val_acc: 0.8791\n",
      "Epoch 67/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4588 - acc: 0.8753 - val_loss: 0.4443 - val_acc: 0.8791\n",
      "Epoch 68/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4564 - acc: 0.8753 - val_loss: 0.4418 - val_acc: 0.8791\n",
      "Epoch 69/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4539 - acc: 0.8753 - val_loss: 0.4397 - val_acc: 0.8791\n",
      "Epoch 70/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4516 - acc: 0.8753 - val_loss: 0.4373 - val_acc: 0.8791\n",
      "Epoch 71/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4492 - acc: 0.8753 - val_loss: 0.4351 - val_acc: 0.8791\n",
      "Epoch 72/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4469 - acc: 0.8753 - val_loss: 0.4328 - val_acc: 0.8791\n",
      "Epoch 73/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4445 - acc: 0.8753 - val_loss: 0.4306 - val_acc: 0.8791\n",
      "Epoch 74/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4422 - acc: 0.8753 - val_loss: 0.4285 - val_acc: 0.8791\n",
      "Epoch 75/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4400 - acc: 0.8753 - val_loss: 0.4263 - val_acc: 0.8791\n",
      "Epoch 76/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4378 - acc: 0.8753 - val_loss: 0.4242 - val_acc: 0.8791\n",
      "Epoch 77/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4356 - acc: 0.8753 - val_loss: 0.4221 - val_acc: 0.8791\n",
      "Epoch 78/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4334 - acc: 0.8753 - val_loss: 0.4200 - val_acc: 0.8791\n",
      "Epoch 79/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4311 - acc: 0.8726 - val_loss: 0.4179 - val_acc: 0.8791\n",
      "Epoch 80/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4290 - acc: 0.8726 - val_loss: 0.4159 - val_acc: 0.8791\n",
      "Epoch 81/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4268 - acc: 0.8726 - val_loss: 0.4140 - val_acc: 0.9011\n",
      "Epoch 82/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4248 - acc: 0.8920 - val_loss: 0.4122 - val_acc: 0.9011\n",
      "Epoch 83/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4229 - acc: 0.8920 - val_loss: 0.4103 - val_acc: 0.9011\n",
      "Epoch 84/500\n",
      "361/361 [==============================] - 0s 33us/step - loss: 0.4209 - acc: 0.8920 - val_loss: 0.4085 - val_acc: 0.9011\n",
      "Epoch 85/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4190 - acc: 0.8920 - val_loss: 0.4067 - val_acc: 0.9011\n",
      "Epoch 86/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.4171 - acc: 0.8920 - val_loss: 0.4050 - val_acc: 0.9011\n",
      "Epoch 87/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.4152 - acc: 0.8920 - val_loss: 0.4034 - val_acc: 0.9011\n",
      "Epoch 88/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.4135 - acc: 0.8920 - val_loss: 0.4017 - val_acc: 0.9011\n",
      "Epoch 89/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.4117 - acc: 0.8920 - val_loss: 0.4000 - val_acc: 0.9011\n",
      "Epoch 90/500\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3635 - acc: 0.937 - 0s 26us/step - loss: 0.4099 - acc: 0.8920 - val_loss: 0.3984 - val_acc: 0.9011\n",
      "Epoch 91/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4081 - acc: 0.8947 - val_loss: 0.3968 - val_acc: 0.9121\n",
      "Epoch 92/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4064 - acc: 0.9141 - val_loss: 0.3951 - val_acc: 0.9121\n",
      "Epoch 93/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4046 - acc: 0.9141 - val_loss: 0.3935 - val_acc: 0.9121\n",
      "Epoch 94/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.4029 - acc: 0.9141 - val_loss: 0.3919 - val_acc: 0.9121\n",
      "Epoch 95/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.4012 - acc: 0.9141 - val_loss: 0.3903 - val_acc: 0.9121\n",
      "Epoch 96/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3995 - acc: 0.9141 - val_loss: 0.3887 - val_acc: 0.9121\n",
      "Epoch 97/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3978 - acc: 0.9141 - val_loss: 0.3872 - val_acc: 0.9121\n",
      "Epoch 98/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.3961 - acc: 0.9141 - val_loss: 0.3857 - val_acc: 0.9121\n",
      "Epoch 99/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3946 - acc: 0.9141 - val_loss: 0.3842 - val_acc: 0.9121\n",
      "Epoch 100/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3930 - acc: 0.9141 - val_loss: 0.3828 - val_acc: 0.9121\n",
      "Epoch 101/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3915 - acc: 0.9141 - val_loss: 0.3813 - val_acc: 0.9121\n",
      "Epoch 102/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3899 - acc: 0.9141 - val_loss: 0.3798 - val_acc: 0.9121\n",
      "Epoch 103/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3883 - acc: 0.9141 - val_loss: 0.3784 - val_acc: 0.9121\n",
      "Epoch 104/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3867 - acc: 0.9141 - val_loss: 0.3770 - val_acc: 0.9121\n",
      "Epoch 105/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3852 - acc: 0.9141 - val_loss: 0.3755 - val_acc: 0.9121\n",
      "Epoch 106/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3836 - acc: 0.9141 - val_loss: 0.3741 - val_acc: 0.9121\n",
      "Epoch 107/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.3821 - acc: 0.9141 - val_loss: 0.3726 - val_acc: 0.9121\n",
      "Epoch 108/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3805 - acc: 0.9141 - val_loss: 0.3712 - val_acc: 0.9121\n",
      "Epoch 109/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3790 - acc: 0.9141 - val_loss: 0.3698 - val_acc: 0.9121\n",
      "Epoch 110/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3774 - acc: 0.9141 - val_loss: 0.3684 - val_acc: 0.9121\n",
      "Epoch 111/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3759 - acc: 0.9141 - val_loss: 0.3670 - val_acc: 0.9121\n",
      "Epoch 112/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3745 - acc: 0.9141 - val_loss: 0.3656 - val_acc: 0.9121\n",
      "Epoch 113/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3730 - acc: 0.9141 - val_loss: 0.3644 - val_acc: 0.9121\n",
      "Epoch 114/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3716 - acc: 0.9141 - val_loss: 0.3631 - val_acc: 0.9121\n",
      "Epoch 115/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3702 - acc: 0.9141 - val_loss: 0.3617 - val_acc: 0.9121\n",
      "Epoch 116/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 25us/step - loss: 0.3688 - acc: 0.9141 - val_loss: 0.3604 - val_acc: 0.9121\n",
      "Epoch 117/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3674 - acc: 0.9141 - val_loss: 0.3592 - val_acc: 0.9121\n",
      "Epoch 118/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3660 - acc: 0.9141 - val_loss: 0.3581 - val_acc: 0.9121\n",
      "Epoch 119/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3648 - acc: 0.9141 - val_loss: 0.3568 - val_acc: 0.9121\n",
      "Epoch 120/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3634 - acc: 0.9141 - val_loss: 0.3555 - val_acc: 0.9121\n",
      "Epoch 121/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3621 - acc: 0.9141 - val_loss: 0.3544 - val_acc: 0.9121\n",
      "Epoch 122/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3608 - acc: 0.9141 - val_loss: 0.3533 - val_acc: 0.9121\n",
      "Epoch 123/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3596 - acc: 0.9141 - val_loss: 0.3520 - val_acc: 0.9121\n",
      "Epoch 124/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.3583 - acc: 0.9169 - val_loss: 0.3508 - val_acc: 0.9121\n",
      "Epoch 125/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3570 - acc: 0.9169 - val_loss: 0.3497 - val_acc: 0.9121\n",
      "Epoch 126/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3557 - acc: 0.9169 - val_loss: 0.3485 - val_acc: 0.9121\n",
      "Epoch 127/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3544 - acc: 0.9169 - val_loss: 0.3474 - val_acc: 0.9121\n",
      "Epoch 128/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3532 - acc: 0.9169 - val_loss: 0.3463 - val_acc: 0.9121\n",
      "Epoch 129/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3519 - acc: 0.9169 - val_loss: 0.3452 - val_acc: 0.9121\n",
      "Epoch 130/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3508 - acc: 0.9169 - val_loss: 0.3441 - val_acc: 0.9121\n",
      "Epoch 131/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3496 - acc: 0.9169 - val_loss: 0.3431 - val_acc: 0.9121\n",
      "Epoch 132/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3484 - acc: 0.9169 - val_loss: 0.3420 - val_acc: 0.9121\n",
      "Epoch 133/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3472 - acc: 0.9169 - val_loss: 0.3409 - val_acc: 0.9121\n",
      "Epoch 134/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3460 - acc: 0.9169 - val_loss: 0.3398 - val_acc: 0.9121\n",
      "Epoch 135/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3449 - acc: 0.9169 - val_loss: 0.3388 - val_acc: 0.9121\n",
      "Epoch 136/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3437 - acc: 0.9169 - val_loss: 0.3378 - val_acc: 0.9121\n",
      "Epoch 137/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3426 - acc: 0.9169 - val_loss: 0.3368 - val_acc: 0.9121\n",
      "Epoch 138/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3415 - acc: 0.9169 - val_loss: 0.3358 - val_acc: 0.9121\n",
      "Epoch 139/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3404 - acc: 0.9169 - val_loss: 0.3347 - val_acc: 0.9121\n",
      "Epoch 140/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3393 - acc: 0.9169 - val_loss: 0.3337 - val_acc: 0.9121\n",
      "Epoch 141/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3382 - acc: 0.9169 - val_loss: 0.3327 - val_acc: 0.9121\n",
      "Epoch 142/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.3371 - acc: 0.9169 - val_loss: 0.3317 - val_acc: 0.9121\n",
      "Epoch 143/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3360 - acc: 0.9169 - val_loss: 0.3308 - val_acc: 0.9121\n",
      "Epoch 144/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3349 - acc: 0.9169 - val_loss: 0.3298 - val_acc: 0.9121\n",
      "Epoch 145/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3339 - acc: 0.9169 - val_loss: 0.3288 - val_acc: 0.9121\n",
      "Epoch 146/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3329 - acc: 0.9169 - val_loss: 0.3279 - val_acc: 0.9121\n",
      "Epoch 147/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.3318 - acc: 0.9169 - val_loss: 0.3270 - val_acc: 0.9121\n",
      "Epoch 148/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3308 - acc: 0.9169 - val_loss: 0.3261 - val_acc: 0.9121\n",
      "Epoch 149/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.3298 - acc: 0.9169 - val_loss: 0.3252 - val_acc: 0.9121\n",
      "Epoch 150/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3288 - acc: 0.9169 - val_loss: 0.3244 - val_acc: 0.9121\n",
      "Epoch 151/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.3279 - acc: 0.9169 - val_loss: 0.3235 - val_acc: 0.9121\n",
      "Epoch 152/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3269 - acc: 0.9169 - val_loss: 0.3227 - val_acc: 0.9121\n",
      "Epoch 153/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3260 - acc: 0.9169 - val_loss: 0.3219 - val_acc: 0.9121\n",
      "Epoch 154/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3251 - acc: 0.9169 - val_loss: 0.3210 - val_acc: 0.9121\n",
      "Epoch 155/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3241 - acc: 0.9169 - val_loss: 0.3202 - val_acc: 0.9121\n",
      "Epoch 156/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3232 - acc: 0.9169 - val_loss: 0.3193 - val_acc: 0.9121\n",
      "Epoch 157/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3223 - acc: 0.9169 - val_loss: 0.3186 - val_acc: 0.9121\n",
      "Epoch 158/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3214 - acc: 0.9169 - val_loss: 0.3178 - val_acc: 0.9121\n",
      "Epoch 159/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3205 - acc: 0.9169 - val_loss: 0.3169 - val_acc: 0.9121\n",
      "Epoch 160/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3196 - acc: 0.9169 - val_loss: 0.3161 - val_acc: 0.9121\n",
      "Epoch 161/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3186 - acc: 0.9169 - val_loss: 0.3153 - val_acc: 0.9121\n",
      "Epoch 162/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3178 - acc: 0.9169 - val_loss: 0.3145 - val_acc: 0.9121\n",
      "Epoch 163/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3169 - acc: 0.9169 - val_loss: 0.3137 - val_acc: 0.9121\n",
      "Epoch 164/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3160 - acc: 0.9169 - val_loss: 0.3130 - val_acc: 0.9121\n",
      "Epoch 165/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3152 - acc: 0.9169 - val_loss: 0.3122 - val_acc: 0.9121\n",
      "Epoch 166/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3143 - acc: 0.9169 - val_loss: 0.3115 - val_acc: 0.9121\n",
      "Epoch 167/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3135 - acc: 0.9169 - val_loss: 0.3108 - val_acc: 0.9121\n",
      "Epoch 168/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.3127 - acc: 0.9169 - val_loss: 0.3100 - val_acc: 0.9121\n",
      "Epoch 169/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3118 - acc: 0.9169 - val_loss: 0.3093 - val_acc: 0.9121\n",
      "Epoch 170/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3110 - acc: 0.9169 - val_loss: 0.3086 - val_acc: 0.9121\n",
      "Epoch 171/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3103 - acc: 0.9169 - val_loss: 0.3079 - val_acc: 0.9121\n",
      "Epoch 172/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3095 - acc: 0.9169 - val_loss: 0.3073 - val_acc: 0.9121\n",
      "Epoch 173/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3087 - acc: 0.9169 - val_loss: 0.3066 - val_acc: 0.9121\n",
      "Epoch 174/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3080 - acc: 0.9169 - val_loss: 0.3059 - val_acc: 0.9121\n",
      "Epoch 175/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.3072 - acc: 0.9169 - val_loss: 0.3053 - val_acc: 0.9121\n",
      "Epoch 176/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 25us/step - loss: 0.3064 - acc: 0.9169 - val_loss: 0.3046 - val_acc: 0.9121\n",
      "Epoch 177/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3057 - acc: 0.9169 - val_loss: 0.3040 - val_acc: 0.9121\n",
      "Epoch 178/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.3049 - acc: 0.9169 - val_loss: 0.3033 - val_acc: 0.9121\n",
      "Epoch 179/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3042 - acc: 0.9169 - val_loss: 0.3026 - val_acc: 0.9121\n",
      "Epoch 180/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3034 - acc: 0.9169 - val_loss: 0.3020 - val_acc: 0.9121\n",
      "Epoch 181/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.3027 - acc: 0.9169 - val_loss: 0.3013 - val_acc: 0.9121\n",
      "Epoch 182/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3019 - acc: 0.9169 - val_loss: 0.3007 - val_acc: 0.9121\n",
      "Epoch 183/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3012 - acc: 0.9169 - val_loss: 0.3000 - val_acc: 0.9121\n",
      "Epoch 184/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.3005 - acc: 0.9169 - val_loss: 0.2994 - val_acc: 0.9121\n",
      "Epoch 185/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2997 - acc: 0.9169 - val_loss: 0.2987 - val_acc: 0.9121\n",
      "Epoch 186/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2990 - acc: 0.9169 - val_loss: 0.2981 - val_acc: 0.9121\n",
      "Epoch 187/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2983 - acc: 0.9169 - val_loss: 0.2975 - val_acc: 0.9121\n",
      "Epoch 188/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2977 - acc: 0.9169 - val_loss: 0.2970 - val_acc: 0.9121\n",
      "Epoch 189/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2970 - acc: 0.9169 - val_loss: 0.2964 - val_acc: 0.9121\n",
      "Epoch 190/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2963 - acc: 0.9169 - val_loss: 0.2959 - val_acc: 0.9121\n",
      "Epoch 191/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2957 - acc: 0.9169 - val_loss: 0.2953 - val_acc: 0.9121\n",
      "Epoch 192/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2950 - acc: 0.9169 - val_loss: 0.2947 - val_acc: 0.9121\n",
      "Epoch 193/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2944 - acc: 0.9169 - val_loss: 0.2941 - val_acc: 0.9121\n",
      "Epoch 194/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2937 - acc: 0.9169 - val_loss: 0.2936 - val_acc: 0.9121\n",
      "Epoch 195/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.2931 - acc: 0.9169 - val_loss: 0.2930 - val_acc: 0.9121\n",
      "Epoch 196/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2925 - acc: 0.9169 - val_loss: 0.2925 - val_acc: 0.9121\n",
      "Epoch 197/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2918 - acc: 0.9169 - val_loss: 0.2920 - val_acc: 0.9121\n",
      "Epoch 198/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2912 - acc: 0.9169 - val_loss: 0.2914 - val_acc: 0.9121\n",
      "Epoch 199/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2906 - acc: 0.9169 - val_loss: 0.2909 - val_acc: 0.9121\n",
      "Epoch 200/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2900 - acc: 0.9169 - val_loss: 0.2904 - val_acc: 0.9121\n",
      "Epoch 201/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2894 - acc: 0.9169 - val_loss: 0.2899 - val_acc: 0.9121\n",
      "Epoch 202/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2888 - acc: 0.9169 - val_loss: 0.2894 - val_acc: 0.9121\n",
      "Epoch 203/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2882 - acc: 0.9169 - val_loss: 0.2889 - val_acc: 0.9121\n",
      "Epoch 204/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2876 - acc: 0.9169 - val_loss: 0.2884 - val_acc: 0.9121\n",
      "Epoch 205/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2871 - acc: 0.9169 - val_loss: 0.2879 - val_acc: 0.9121\n",
      "Epoch 206/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2865 - acc: 0.9169 - val_loss: 0.2874 - val_acc: 0.9121\n",
      "Epoch 207/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2859 - acc: 0.9169 - val_loss: 0.2869 - val_acc: 0.9121\n",
      "Epoch 208/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2854 - acc: 0.9169 - val_loss: 0.2864 - val_acc: 0.9121\n",
      "Epoch 209/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2848 - acc: 0.9169 - val_loss: 0.2859 - val_acc: 0.9121\n",
      "Epoch 210/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2843 - acc: 0.9169 - val_loss: 0.2855 - val_acc: 0.9121\n",
      "Epoch 211/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2837 - acc: 0.9169 - val_loss: 0.2850 - val_acc: 0.9121\n",
      "Epoch 212/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2832 - acc: 0.9169 - val_loss: 0.2845 - val_acc: 0.9121\n",
      "Epoch 213/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2827 - acc: 0.9169 - val_loss: 0.2840 - val_acc: 0.9121\n",
      "Epoch 214/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2821 - acc: 0.9169 - val_loss: 0.2836 - val_acc: 0.9121\n",
      "Epoch 215/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2816 - acc: 0.9169 - val_loss: 0.2832 - val_acc: 0.9121\n",
      "Epoch 216/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2811 - acc: 0.9169 - val_loss: 0.2827 - val_acc: 0.9121\n",
      "Epoch 217/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2806 - acc: 0.9169 - val_loss: 0.2822 - val_acc: 0.9121\n",
      "Epoch 218/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2800 - acc: 0.9169 - val_loss: 0.2818 - val_acc: 0.9121\n",
      "Epoch 219/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2795 - acc: 0.9169 - val_loss: 0.2813 - val_acc: 0.9121\n",
      "Epoch 220/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2790 - acc: 0.9169 - val_loss: 0.2809 - val_acc: 0.9121\n",
      "Epoch 221/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2785 - acc: 0.9169 - val_loss: 0.2805 - val_acc: 0.9121\n",
      "Epoch 222/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2780 - acc: 0.9169 - val_loss: 0.2801 - val_acc: 0.9121\n",
      "Epoch 223/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2775 - acc: 0.9169 - val_loss: 0.2796 - val_acc: 0.9121\n",
      "Epoch 224/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.2770 - acc: 0.9169 - val_loss: 0.2792 - val_acc: 0.9121\n",
      "Epoch 225/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2766 - acc: 0.9169 - val_loss: 0.2788 - val_acc: 0.9121\n",
      "Epoch 226/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2761 - acc: 0.9169 - val_loss: 0.2784 - val_acc: 0.9121\n",
      "Epoch 227/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2756 - acc: 0.9169 - val_loss: 0.2780 - val_acc: 0.9121\n",
      "Epoch 228/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2751 - acc: 0.9169 - val_loss: 0.2776 - val_acc: 0.9121\n",
      "Epoch 229/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2746 - acc: 0.9169 - val_loss: 0.2772 - val_acc: 0.9121\n",
      "Epoch 230/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2742 - acc: 0.9169 - val_loss: 0.2769 - val_acc: 0.9121\n",
      "Epoch 231/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2738 - acc: 0.9169 - val_loss: 0.2765 - val_acc: 0.9121\n",
      "Epoch 232/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2733 - acc: 0.9169 - val_loss: 0.2761 - val_acc: 0.9121\n",
      "Epoch 233/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2729 - acc: 0.9169 - val_loss: 0.2757 - val_acc: 0.9121\n",
      "Epoch 234/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2724 - acc: 0.9169 - val_loss: 0.2754 - val_acc: 0.9121\n",
      "Epoch 235/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2720 - acc: 0.9169 - val_loss: 0.2750 - val_acc: 0.9121\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 28us/step - loss: 0.2715 - acc: 0.9169 - val_loss: 0.2746 - val_acc: 0.9121\n",
      "Epoch 237/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2711 - acc: 0.9169 - val_loss: 0.2743 - val_acc: 0.9121\n",
      "Epoch 238/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2707 - acc: 0.9169 - val_loss: 0.2739 - val_acc: 0.9121\n",
      "Epoch 239/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2702 - acc: 0.9169 - val_loss: 0.2736 - val_acc: 0.9121\n",
      "Epoch 240/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2698 - acc: 0.9169 - val_loss: 0.2732 - val_acc: 0.9121\n",
      "Epoch 241/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2694 - acc: 0.9169 - val_loss: 0.2728 - val_acc: 0.9121\n",
      "Epoch 242/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2690 - acc: 0.9169 - val_loss: 0.2725 - val_acc: 0.9121\n",
      "Epoch 243/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2686 - acc: 0.9169 - val_loss: 0.2721 - val_acc: 0.9121\n",
      "Epoch 244/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2681 - acc: 0.9169 - val_loss: 0.2718 - val_acc: 0.9121\n",
      "Epoch 245/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2677 - acc: 0.9169 - val_loss: 0.2714 - val_acc: 0.9121\n",
      "Epoch 246/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2673 - acc: 0.9169 - val_loss: 0.2711 - val_acc: 0.9121\n",
      "Epoch 247/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2669 - acc: 0.9169 - val_loss: 0.2708 - val_acc: 0.9121\n",
      "Epoch 248/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2665 - acc: 0.9169 - val_loss: 0.2704 - val_acc: 0.9121\n",
      "Epoch 249/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2661 - acc: 0.9169 - val_loss: 0.2701 - val_acc: 0.9121\n",
      "Epoch 250/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2657 - acc: 0.9169 - val_loss: 0.2697 - val_acc: 0.9121\n",
      "Epoch 251/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2653 - acc: 0.9169 - val_loss: 0.2694 - val_acc: 0.9121\n",
      "Epoch 252/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2649 - acc: 0.9169 - val_loss: 0.2691 - val_acc: 0.9121\n",
      "Epoch 253/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2645 - acc: 0.9169 - val_loss: 0.2687 - val_acc: 0.9121\n",
      "Epoch 254/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2641 - acc: 0.9169 - val_loss: 0.2684 - val_acc: 0.9121\n",
      "Epoch 255/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2637 - acc: 0.9169 - val_loss: 0.2681 - val_acc: 0.9121\n",
      "Epoch 256/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2634 - acc: 0.9169 - val_loss: 0.2678 - val_acc: 0.9121\n",
      "Epoch 257/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2630 - acc: 0.9169 - val_loss: 0.2675 - val_acc: 0.9121\n",
      "Epoch 258/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2626 - acc: 0.9169 - val_loss: 0.2672 - val_acc: 0.9121\n",
      "Epoch 259/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2622 - acc: 0.9169 - val_loss: 0.2669 - val_acc: 0.9121\n",
      "Epoch 260/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2619 - acc: 0.9169 - val_loss: 0.2666 - val_acc: 0.9121\n",
      "Epoch 261/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2615 - acc: 0.9169 - val_loss: 0.2663 - val_acc: 0.9121\n",
      "Epoch 262/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2611 - acc: 0.9169 - val_loss: 0.2660 - val_acc: 0.9121\n",
      "Epoch 263/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2608 - acc: 0.9169 - val_loss: 0.2657 - val_acc: 0.9121\n",
      "Epoch 264/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2604 - acc: 0.9169 - val_loss: 0.2654 - val_acc: 0.9121\n",
      "Epoch 265/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2600 - acc: 0.9169 - val_loss: 0.2651 - val_acc: 0.9121\n",
      "Epoch 266/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2597 - acc: 0.9169 - val_loss: 0.2648 - val_acc: 0.9121\n",
      "Epoch 267/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2593 - acc: 0.9169 - val_loss: 0.2645 - val_acc: 0.9121\n",
      "Epoch 268/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2590 - acc: 0.9169 - val_loss: 0.2642 - val_acc: 0.9121\n",
      "Epoch 269/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2586 - acc: 0.9169 - val_loss: 0.2640 - val_acc: 0.9121\n",
      "Epoch 270/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2583 - acc: 0.9169 - val_loss: 0.2637 - val_acc: 0.9121\n",
      "Epoch 271/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2580 - acc: 0.9169 - val_loss: 0.2634 - val_acc: 0.9121\n",
      "Epoch 272/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2576 - acc: 0.9169 - val_loss: 0.2631 - val_acc: 0.9121\n",
      "Epoch 273/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2573 - acc: 0.9169 - val_loss: 0.2628 - val_acc: 0.9121\n",
      "Epoch 274/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2569 - acc: 0.9169 - val_loss: 0.2625 - val_acc: 0.9121\n",
      "Epoch 275/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2566 - acc: 0.9169 - val_loss: 0.2623 - val_acc: 0.9121\n",
      "Epoch 276/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2562 - acc: 0.9169 - val_loss: 0.2620 - val_acc: 0.9121\n",
      "Epoch 277/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2559 - acc: 0.9169 - val_loss: 0.2617 - val_acc: 0.9121\n",
      "Epoch 278/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2556 - acc: 0.9169 - val_loss: 0.2615 - val_acc: 0.9121\n",
      "Epoch 279/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2553 - acc: 0.9169 - val_loss: 0.2612 - val_acc: 0.9121\n",
      "Epoch 280/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2549 - acc: 0.9169 - val_loss: 0.2609 - val_acc: 0.9121\n",
      "Epoch 281/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2546 - acc: 0.9169 - val_loss: 0.2607 - val_acc: 0.9121\n",
      "Epoch 282/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2543 - acc: 0.9169 - val_loss: 0.2604 - val_acc: 0.9121\n",
      "Epoch 283/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2540 - acc: 0.9169 - val_loss: 0.2602 - val_acc: 0.9121\n",
      "Epoch 284/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2537 - acc: 0.9169 - val_loss: 0.2599 - val_acc: 0.9121\n",
      "Epoch 285/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2534 - acc: 0.9169 - val_loss: 0.2597 - val_acc: 0.9121\n",
      "Epoch 286/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2531 - acc: 0.9169 - val_loss: 0.2594 - val_acc: 0.9121\n",
      "Epoch 287/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2528 - acc: 0.9169 - val_loss: 0.2592 - val_acc: 0.9121\n",
      "Epoch 288/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2525 - acc: 0.9169 - val_loss: 0.2589 - val_acc: 0.9121\n",
      "Epoch 289/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2522 - acc: 0.9169 - val_loss: 0.2587 - val_acc: 0.9121\n",
      "Epoch 290/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2519 - acc: 0.9169 - val_loss: 0.2585 - val_acc: 0.9121\n",
      "Epoch 291/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2516 - acc: 0.9169 - val_loss: 0.2582 - val_acc: 0.9121\n",
      "Epoch 292/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2513 - acc: 0.9169 - val_loss: 0.2580 - val_acc: 0.9121\n",
      "Epoch 293/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2510 - acc: 0.9169 - val_loss: 0.2577 - val_acc: 0.9121\n",
      "Epoch 294/500\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.2465 - acc: 0.937 - 0s 25us/step - loss: 0.2507 - acc: 0.9169 - val_loss: 0.2575 - val_acc: 0.9121\n",
      "Epoch 295/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2504 - acc: 0.9169 - val_loss: 0.2573 - val_acc: 0.9121\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 25us/step - loss: 0.2501 - acc: 0.9169 - val_loss: 0.2570 - val_acc: 0.9121\n",
      "Epoch 297/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2498 - acc: 0.9169 - val_loss: 0.2568 - val_acc: 0.9121\n",
      "Epoch 298/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2495 - acc: 0.9169 - val_loss: 0.2566 - val_acc: 0.9121\n",
      "Epoch 299/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2492 - acc: 0.9169 - val_loss: 0.2563 - val_acc: 0.9121\n",
      "Epoch 300/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2489 - acc: 0.9169 - val_loss: 0.2561 - val_acc: 0.9121\n",
      "Epoch 301/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2486 - acc: 0.9169 - val_loss: 0.2559 - val_acc: 0.9121\n",
      "Epoch 302/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2483 - acc: 0.9169 - val_loss: 0.2557 - val_acc: 0.9121\n",
      "Epoch 303/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2480 - acc: 0.9169 - val_loss: 0.2555 - val_acc: 0.9121\n",
      "Epoch 304/500\n",
      "361/361 [==============================] - 0s 23us/step - loss: 0.2477 - acc: 0.9169 - val_loss: 0.2552 - val_acc: 0.9121\n",
      "Epoch 305/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2475 - acc: 0.9169 - val_loss: 0.2550 - val_acc: 0.9121\n",
      "Epoch 306/500\n",
      "361/361 [==============================] - 0s 24us/step - loss: 0.2472 - acc: 0.9169 - val_loss: 0.2548 - val_acc: 0.9121\n",
      "Epoch 307/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2469 - acc: 0.9169 - val_loss: 0.2546 - val_acc: 0.9121\n",
      "Epoch 308/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.2466 - acc: 0.9169 - val_loss: 0.2544 - val_acc: 0.9121\n",
      "Epoch 309/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2463 - acc: 0.9169 - val_loss: 0.2541 - val_acc: 0.9121\n",
      "Epoch 310/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2461 - acc: 0.9169 - val_loss: 0.2539 - val_acc: 0.9121\n",
      "Epoch 311/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2458 - acc: 0.9169 - val_loss: 0.2537 - val_acc: 0.9121\n",
      "Epoch 312/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2455 - acc: 0.9169 - val_loss: 0.2535 - val_acc: 0.9121\n",
      "Epoch 313/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.2452 - acc: 0.9169 - val_loss: 0.2533 - val_acc: 0.9121\n",
      "Epoch 314/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2449 - acc: 0.9169 - val_loss: 0.2531 - val_acc: 0.9121\n",
      "Epoch 315/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2447 - acc: 0.9169 - val_loss: 0.2529 - val_acc: 0.9121\n",
      "Epoch 316/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2444 - acc: 0.9169 - val_loss: 0.2527 - val_acc: 0.9121\n",
      "Epoch 317/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2442 - acc: 0.9169 - val_loss: 0.2525 - val_acc: 0.9121\n",
      "Epoch 318/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2439 - acc: 0.9169 - val_loss: 0.2523 - val_acc: 0.9121\n",
      "Epoch 319/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2436 - acc: 0.9169 - val_loss: 0.2521 - val_acc: 0.9121\n",
      "Epoch 320/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2434 - acc: 0.9169 - val_loss: 0.2519 - val_acc: 0.9121\n",
      "Epoch 321/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2431 - acc: 0.9169 - val_loss: 0.2516 - val_acc: 0.9121\n",
      "Epoch 322/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2428 - acc: 0.9169 - val_loss: 0.2514 - val_acc: 0.9121\n",
      "Epoch 323/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2426 - acc: 0.9169 - val_loss: 0.2512 - val_acc: 0.9121\n",
      "Epoch 324/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2423 - acc: 0.9169 - val_loss: 0.2510 - val_acc: 0.9121\n",
      "Epoch 325/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2421 - acc: 0.9169 - val_loss: 0.2509 - val_acc: 0.9121\n",
      "Epoch 326/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2418 - acc: 0.9169 - val_loss: 0.2507 - val_acc: 0.9121\n",
      "Epoch 327/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2416 - acc: 0.9169 - val_loss: 0.2505 - val_acc: 0.9121\n",
      "Epoch 328/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2413 - acc: 0.9169 - val_loss: 0.2503 - val_acc: 0.9121\n",
      "Epoch 329/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2411 - acc: 0.9169 - val_loss: 0.2501 - val_acc: 0.9121\n",
      "Epoch 330/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2408 - acc: 0.9169 - val_loss: 0.2499 - val_acc: 0.9121\n",
      "Epoch 331/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2406 - acc: 0.9169 - val_loss: 0.2497 - val_acc: 0.9121\n",
      "Epoch 332/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2403 - acc: 0.9169 - val_loss: 0.2495 - val_acc: 0.9121\n",
      "Epoch 333/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2401 - acc: 0.9169 - val_loss: 0.2494 - val_acc: 0.9121\n",
      "Epoch 334/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2399 - acc: 0.9169 - val_loss: 0.2492 - val_acc: 0.9121\n",
      "Epoch 335/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2396 - acc: 0.9169 - val_loss: 0.2490 - val_acc: 0.9121\n",
      "Epoch 336/500\n",
      "361/361 [==============================] - 0s 23us/step - loss: 0.2394 - acc: 0.9169 - val_loss: 0.2489 - val_acc: 0.9121\n",
      "Epoch 337/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2392 - acc: 0.9169 - val_loss: 0.2487 - val_acc: 0.9121\n",
      "Epoch 338/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2390 - acc: 0.9169 - val_loss: 0.2486 - val_acc: 0.9121\n",
      "Epoch 339/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2388 - acc: 0.9169 - val_loss: 0.2484 - val_acc: 0.9121\n",
      "Epoch 340/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2386 - acc: 0.9169 - val_loss: 0.2482 - val_acc: 0.9121\n",
      "Epoch 341/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2384 - acc: 0.9169 - val_loss: 0.2481 - val_acc: 0.9121\n",
      "Epoch 342/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2382 - acc: 0.9169 - val_loss: 0.2479 - val_acc: 0.9121\n",
      "Epoch 343/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2380 - acc: 0.9169 - val_loss: 0.2478 - val_acc: 0.9121\n",
      "Epoch 344/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2378 - acc: 0.9169 - val_loss: 0.2476 - val_acc: 0.9121\n",
      "Epoch 345/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2376 - acc: 0.9169 - val_loss: 0.2474 - val_acc: 0.9121\n",
      "Epoch 346/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2374 - acc: 0.9169 - val_loss: 0.2473 - val_acc: 0.9121\n",
      "Epoch 347/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2372 - acc: 0.9169 - val_loss: 0.2471 - val_acc: 0.9121\n",
      "Epoch 348/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2370 - acc: 0.9169 - val_loss: 0.2470 - val_acc: 0.9121\n",
      "Epoch 349/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2368 - acc: 0.9169 - val_loss: 0.2469 - val_acc: 0.9121\n",
      "Epoch 350/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2366 - acc: 0.9169 - val_loss: 0.2467 - val_acc: 0.9121\n",
      "Epoch 351/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2364 - acc: 0.9169 - val_loss: 0.2465 - val_acc: 0.9121\n",
      "Epoch 352/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2362 - acc: 0.9169 - val_loss: 0.2464 - val_acc: 0.9121\n",
      "Epoch 353/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2360 - acc: 0.9169 - val_loss: 0.2463 - val_acc: 0.9121\n",
      "Epoch 354/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2358 - acc: 0.9169 - val_loss: 0.2461 - val_acc: 0.9121\n",
      "Epoch 355/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2356 - acc: 0.9169 - val_loss: 0.2460 - val_acc: 0.9121\n",
      "Epoch 356/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 0s 25us/step - loss: 0.2354 - acc: 0.9169 - val_loss: 0.2458 - val_acc: 0.9121\n",
      "Epoch 357/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2353 - acc: 0.9169 - val_loss: 0.2457 - val_acc: 0.9121\n",
      "Epoch 358/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2351 - acc: 0.9169 - val_loss: 0.2456 - val_acc: 0.9121\n",
      "Epoch 359/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2349 - acc: 0.9169 - val_loss: 0.2454 - val_acc: 0.9121\n",
      "Epoch 360/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2347 - acc: 0.9169 - val_loss: 0.2453 - val_acc: 0.9121\n",
      "Epoch 361/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2345 - acc: 0.9169 - val_loss: 0.2452 - val_acc: 0.9121\n",
      "Epoch 362/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2343 - acc: 0.9169 - val_loss: 0.2450 - val_acc: 0.9121\n",
      "Epoch 363/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2342 - acc: 0.9169 - val_loss: 0.2449 - val_acc: 0.9121\n",
      "Epoch 364/500\n",
      "361/361 [==============================] - 0s 23us/step - loss: 0.2340 - acc: 0.9169 - val_loss: 0.2448 - val_acc: 0.9121\n",
      "Epoch 365/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2338 - acc: 0.9169 - val_loss: 0.2446 - val_acc: 0.9121\n",
      "Epoch 366/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2336 - acc: 0.9169 - val_loss: 0.2445 - val_acc: 0.9121\n",
      "Epoch 367/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2335 - acc: 0.9169 - val_loss: 0.2444 - val_acc: 0.9121\n",
      "Epoch 368/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2333 - acc: 0.9169 - val_loss: 0.2442 - val_acc: 0.9121\n",
      "Epoch 369/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2331 - acc: 0.9169 - val_loss: 0.2441 - val_acc: 0.9121\n",
      "Epoch 370/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2329 - acc: 0.9169 - val_loss: 0.2440 - val_acc: 0.9121\n",
      "Epoch 371/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2328 - acc: 0.9169 - val_loss: 0.2438 - val_acc: 0.9121\n",
      "Epoch 372/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2326 - acc: 0.9169 - val_loss: 0.2437 - val_acc: 0.9121\n",
      "Epoch 373/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2324 - acc: 0.9169 - val_loss: 0.2436 - val_acc: 0.9121\n",
      "Epoch 374/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2322 - acc: 0.9169 - val_loss: 0.2434 - val_acc: 0.9121\n",
      "Epoch 375/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2321 - acc: 0.9169 - val_loss: 0.2433 - val_acc: 0.9121\n",
      "Epoch 376/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2319 - acc: 0.9169 - val_loss: 0.2432 - val_acc: 0.9121\n",
      "Epoch 377/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2318 - acc: 0.9169 - val_loss: 0.2431 - val_acc: 0.9121\n",
      "Epoch 378/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2316 - acc: 0.9169 - val_loss: 0.2430 - val_acc: 0.9121\n",
      "Epoch 379/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2315 - acc: 0.9169 - val_loss: 0.2429 - val_acc: 0.9121\n",
      "Epoch 380/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2313 - acc: 0.9169 - val_loss: 0.2427 - val_acc: 0.9121\n",
      "Epoch 381/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2311 - acc: 0.9169 - val_loss: 0.2426 - val_acc: 0.9121\n",
      "Epoch 382/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2310 - acc: 0.9169 - val_loss: 0.2425 - val_acc: 0.9121\n",
      "Epoch 383/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2308 - acc: 0.9169 - val_loss: 0.2424 - val_acc: 0.9121\n",
      "Epoch 384/500\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.2014 - acc: 0.937 - 0s 25us/step - loss: 0.2307 - acc: 0.9169 - val_loss: 0.2423 - val_acc: 0.9121\n",
      "Epoch 385/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2305 - acc: 0.9169 - val_loss: 0.2422 - val_acc: 0.9121\n",
      "Epoch 386/500\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.2496 - acc: 0.937 - 0s 25us/step - loss: 0.2303 - acc: 0.9169 - val_loss: 0.2420 - val_acc: 0.9121\n",
      "Epoch 387/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2302 - acc: 0.9169 - val_loss: 0.2419 - val_acc: 0.9121\n",
      "Epoch 388/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2300 - acc: 0.9169 - val_loss: 0.2418 - val_acc: 0.9121\n",
      "Epoch 389/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2298 - acc: 0.9169 - val_loss: 0.2417 - val_acc: 0.9121\n",
      "Epoch 390/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2297 - acc: 0.9169 - val_loss: 0.2415 - val_acc: 0.9121\n",
      "Epoch 391/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2295 - acc: 0.9169 - val_loss: 0.2414 - val_acc: 0.9121\n",
      "Epoch 392/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2294 - acc: 0.9169 - val_loss: 0.2413 - val_acc: 0.9121\n",
      "Epoch 393/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2292 - acc: 0.9169 - val_loss: 0.2412 - val_acc: 0.9121\n",
      "Epoch 394/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2291 - acc: 0.9169 - val_loss: 0.2411 - val_acc: 0.9121\n",
      "Epoch 395/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2289 - acc: 0.9169 - val_loss: 0.2409 - val_acc: 0.9121\n",
      "Epoch 396/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2288 - acc: 0.9169 - val_loss: 0.2408 - val_acc: 0.9121\n",
      "Epoch 397/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2286 - acc: 0.9169 - val_loss: 0.2407 - val_acc: 0.9121\n",
      "Epoch 398/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2285 - acc: 0.9169 - val_loss: 0.2406 - val_acc: 0.9121\n",
      "Epoch 399/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2283 - acc: 0.9169 - val_loss: 0.2405 - val_acc: 0.9121\n",
      "Epoch 400/500\n",
      "361/361 [==============================] - 0s 33us/step - loss: 0.2282 - acc: 0.9169 - val_loss: 0.2404 - val_acc: 0.9121\n",
      "Epoch 401/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2280 - acc: 0.9169 - val_loss: 0.2403 - val_acc: 0.9121\n",
      "Epoch 402/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2279 - acc: 0.9169 - val_loss: 0.2402 - val_acc: 0.9121\n",
      "Epoch 403/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2277 - acc: 0.9169 - val_loss: 0.2401 - val_acc: 0.9121\n",
      "Epoch 404/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2276 - acc: 0.9169 - val_loss: 0.2400 - val_acc: 0.9121\n",
      "Epoch 405/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2275 - acc: 0.9169 - val_loss: 0.2399 - val_acc: 0.9121\n",
      "Epoch 406/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2273 - acc: 0.9169 - val_loss: 0.2398 - val_acc: 0.9121\n",
      "Epoch 407/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2272 - acc: 0.9169 - val_loss: 0.2397 - val_acc: 0.9121\n",
      "Epoch 408/500\n",
      "361/361 [==============================] - 0s 30us/step - loss: 0.2271 - acc: 0.9169 - val_loss: 0.2396 - val_acc: 0.9121\n",
      "Epoch 409/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.2269 - acc: 0.9114 - val_loss: 0.2395 - val_acc: 0.9121\n",
      "Epoch 410/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2268 - acc: 0.9114 - val_loss: 0.2394 - val_acc: 0.9121\n",
      "Epoch 411/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2267 - acc: 0.9114 - val_loss: 0.2393 - val_acc: 0.9121\n",
      "Epoch 412/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2265 - acc: 0.9114 - val_loss: 0.2392 - val_acc: 0.9121\n",
      "Epoch 413/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2264 - acc: 0.9114 - val_loss: 0.2391 - val_acc: 0.9121\n",
      "Epoch 414/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2262 - acc: 0.9114 - val_loss: 0.2390 - val_acc: 0.9121\n",
      "Epoch 415/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2261 - acc: 0.9114 - val_loss: 0.2389 - val_acc: 0.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2260 - acc: 0.9114 - val_loss: 0.2388 - val_acc: 0.9121\n",
      "Epoch 417/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2259 - acc: 0.9114 - val_loss: 0.2387 - val_acc: 0.9121\n",
      "Epoch 418/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2257 - acc: 0.9114 - val_loss: 0.2386 - val_acc: 0.9121\n",
      "Epoch 419/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2256 - acc: 0.9114 - val_loss: 0.2385 - val_acc: 0.9121\n",
      "Epoch 420/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2255 - acc: 0.9114 - val_loss: 0.2384 - val_acc: 0.9121\n",
      "Epoch 421/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2253 - acc: 0.9114 - val_loss: 0.2383 - val_acc: 0.9121\n",
      "Epoch 422/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2252 - acc: 0.9114 - val_loss: 0.2382 - val_acc: 0.9121\n",
      "Epoch 423/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2251 - acc: 0.9114 - val_loss: 0.2381 - val_acc: 0.9121\n",
      "Epoch 424/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2249 - acc: 0.9114 - val_loss: 0.2381 - val_acc: 0.9121\n",
      "Epoch 425/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2248 - acc: 0.9114 - val_loss: 0.2380 - val_acc: 0.9121\n",
      "Epoch 426/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2247 - acc: 0.9114 - val_loss: 0.2379 - val_acc: 0.9121\n",
      "Epoch 427/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.2246 - acc: 0.9114 - val_loss: 0.2378 - val_acc: 0.9121\n",
      "Epoch 428/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2244 - acc: 0.9114 - val_loss: 0.2377 - val_acc: 0.9121\n",
      "Epoch 429/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2243 - acc: 0.9114 - val_loss: 0.2376 - val_acc: 0.9121\n",
      "Epoch 430/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2242 - acc: 0.9114 - val_loss: 0.2375 - val_acc: 0.9121\n",
      "Epoch 431/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2241 - acc: 0.9114 - val_loss: 0.2374 - val_acc: 0.9121\n",
      "Epoch 432/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2239 - acc: 0.9114 - val_loss: 0.2373 - val_acc: 0.9121\n",
      "Epoch 433/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2238 - acc: 0.9114 - val_loss: 0.2372 - val_acc: 0.9121\n",
      "Epoch 434/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2237 - acc: 0.9114 - val_loss: 0.2371 - val_acc: 0.9121\n",
      "Epoch 435/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2236 - acc: 0.9114 - val_loss: 0.2370 - val_acc: 0.9121\n",
      "Epoch 436/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2234 - acc: 0.9114 - val_loss: 0.2369 - val_acc: 0.9121\n",
      "Epoch 437/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2233 - acc: 0.9114 - val_loss: 0.2369 - val_acc: 0.9121\n",
      "Epoch 438/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2232 - acc: 0.9114 - val_loss: 0.2368 - val_acc: 0.9121\n",
      "Epoch 439/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2231 - acc: 0.9114 - val_loss: 0.2367 - val_acc: 0.9121\n",
      "Epoch 440/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2229 - acc: 0.9114 - val_loss: 0.2366 - val_acc: 0.9121\n",
      "Epoch 441/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2228 - acc: 0.9114 - val_loss: 0.2365 - val_acc: 0.9121\n",
      "Epoch 442/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2227 - acc: 0.9114 - val_loss: 0.2364 - val_acc: 0.9121\n",
      "Epoch 443/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2226 - acc: 0.9114 - val_loss: 0.2363 - val_acc: 0.9121\n",
      "Epoch 444/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2224 - acc: 0.9114 - val_loss: 0.2362 - val_acc: 0.9121\n",
      "Epoch 445/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2223 - acc: 0.9114 - val_loss: 0.2361 - val_acc: 0.9121\n",
      "Epoch 446/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2222 - acc: 0.9114 - val_loss: 0.2360 - val_acc: 0.9121\n",
      "Epoch 447/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2221 - acc: 0.9114 - val_loss: 0.2360 - val_acc: 0.9121\n",
      "Epoch 448/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2220 - acc: 0.9114 - val_loss: 0.2359 - val_acc: 0.9121\n",
      "Epoch 449/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2219 - acc: 0.9114 - val_loss: 0.2358 - val_acc: 0.9121\n",
      "Epoch 450/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2218 - acc: 0.9114 - val_loss: 0.2357 - val_acc: 0.9121\n",
      "Epoch 451/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2216 - acc: 0.9114 - val_loss: 0.2356 - val_acc: 0.9121\n",
      "Epoch 452/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2215 - acc: 0.9114 - val_loss: 0.2356 - val_acc: 0.9121\n",
      "Epoch 453/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2214 - acc: 0.9114 - val_loss: 0.2355 - val_acc: 0.9121\n",
      "Epoch 454/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2213 - acc: 0.9114 - val_loss: 0.2354 - val_acc: 0.9121\n",
      "Epoch 455/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2212 - acc: 0.9114 - val_loss: 0.2353 - val_acc: 0.9121\n",
      "Epoch 456/500\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.1963 - acc: 0.906 - 0s 25us/step - loss: 0.2211 - acc: 0.9114 - val_loss: 0.2352 - val_acc: 0.9121\n",
      "Epoch 457/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2209 - acc: 0.9114 - val_loss: 0.2352 - val_acc: 0.9121\n",
      "Epoch 458/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2208 - acc: 0.9114 - val_loss: 0.2351 - val_acc: 0.9121\n",
      "Epoch 459/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2207 - acc: 0.9114 - val_loss: 0.2350 - val_acc: 0.9121\n",
      "Epoch 460/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2206 - acc: 0.9114 - val_loss: 0.2350 - val_acc: 0.9121\n",
      "Epoch 461/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2205 - acc: 0.9114 - val_loss: 0.2349 - val_acc: 0.9121\n",
      "Epoch 462/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2204 - acc: 0.9114 - val_loss: 0.2348 - val_acc: 0.9121\n",
      "Epoch 463/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2203 - acc: 0.9114 - val_loss: 0.2347 - val_acc: 0.9121\n",
      "Epoch 464/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2202 - acc: 0.9114 - val_loss: 0.2346 - val_acc: 0.9121\n",
      "Epoch 465/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2201 - acc: 0.9114 - val_loss: 0.2346 - val_acc: 0.9121\n",
      "Epoch 466/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2200 - acc: 0.9114 - val_loss: 0.2345 - val_acc: 0.9121\n",
      "Epoch 467/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2199 - acc: 0.9114 - val_loss: 0.2344 - val_acc: 0.9121\n",
      "Epoch 468/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2198 - acc: 0.9114 - val_loss: 0.2344 - val_acc: 0.9121\n",
      "Epoch 469/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2197 - acc: 0.9114 - val_loss: 0.2343 - val_acc: 0.9121\n",
      "Epoch 470/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2196 - acc: 0.9114 - val_loss: 0.2342 - val_acc: 0.9121\n",
      "Epoch 471/500\n",
      "361/361 [==============================] - 0s 26us/step - loss: 0.2195 - acc: 0.9114 - val_loss: 0.2341 - val_acc: 0.9121\n",
      "Epoch 472/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2194 - acc: 0.9114 - val_loss: 0.2340 - val_acc: 0.9121\n",
      "Epoch 473/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2192 - acc: 0.9114 - val_loss: 0.2340 - val_acc: 0.9121\n",
      "Epoch 474/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2192 - acc: 0.9114 - val_loss: 0.2339 - val_acc: 0.9121\n",
      "Epoch 475/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2190 - acc: 0.9114 - val_loss: 0.2338 - val_acc: 0.9121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2189 - acc: 0.9114 - val_loss: 0.2338 - val_acc: 0.9121\n",
      "Epoch 477/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2188 - acc: 0.9114 - val_loss: 0.2337 - val_acc: 0.9121\n",
      "Epoch 478/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2187 - acc: 0.9114 - val_loss: 0.2336 - val_acc: 0.9121\n",
      "Epoch 479/500\n",
      "361/361 [==============================] - 0s 27us/step - loss: 0.2186 - acc: 0.9114 - val_loss: 0.2335 - val_acc: 0.9121\n",
      "Epoch 480/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2185 - acc: 0.9114 - val_loss: 0.2335 - val_acc: 0.9121\n",
      "Epoch 481/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2184 - acc: 0.9114 - val_loss: 0.2334 - val_acc: 0.9121\n",
      "Epoch 482/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2183 - acc: 0.9114 - val_loss: 0.2333 - val_acc: 0.9121\n",
      "Epoch 483/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2182 - acc: 0.9114 - val_loss: 0.2333 - val_acc: 0.9121\n",
      "Epoch 484/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2181 - acc: 0.9114 - val_loss: 0.2332 - val_acc: 0.9121\n",
      "Epoch 485/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2181 - acc: 0.9114 - val_loss: 0.2331 - val_acc: 0.9121\n",
      "Epoch 486/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2179 - acc: 0.9114 - val_loss: 0.2330 - val_acc: 0.9121\n",
      "Epoch 487/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2178 - acc: 0.9114 - val_loss: 0.2330 - val_acc: 0.9121\n",
      "Epoch 488/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2177 - acc: 0.9114 - val_loss: 0.2329 - val_acc: 0.9121\n",
      "Epoch 489/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2176 - acc: 0.9114 - val_loss: 0.2328 - val_acc: 0.9121\n",
      "Epoch 490/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2175 - acc: 0.9114 - val_loss: 0.2328 - val_acc: 0.9121\n",
      "Epoch 491/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2174 - acc: 0.9114 - val_loss: 0.2327 - val_acc: 0.9121\n",
      "Epoch 492/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2174 - acc: 0.9114 - val_loss: 0.2326 - val_acc: 0.9121\n",
      "Epoch 493/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2173 - acc: 0.9114 - val_loss: 0.2326 - val_acc: 0.9121\n",
      "Epoch 494/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2172 - acc: 0.9114 - val_loss: 0.2325 - val_acc: 0.9121\n",
      "Epoch 495/500\n",
      "361/361 [==============================] - 0s 22us/step - loss: 0.2171 - acc: 0.9114 - val_loss: 0.2324 - val_acc: 0.9121\n",
      "Epoch 496/500\n",
      "361/361 [==============================] - 0s 25us/step - loss: 0.2170 - acc: 0.9114 - val_loss: 0.2323 - val_acc: 0.9121\n",
      "Epoch 497/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2169 - acc: 0.9114 - val_loss: 0.2323 - val_acc: 0.9121\n",
      "Epoch 498/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2168 - acc: 0.9114 - val_loss: 0.2322 - val_acc: 0.9121\n",
      "Epoch 499/500\n",
      "361/361 [==============================] - 0s 23us/step - loss: 0.2167 - acc: 0.9114 - val_loss: 0.2321 - val_acc: 0.9121\n",
      "Epoch 500/500\n",
      "361/361 [==============================] - 0s 28us/step - loss: 0.2166 - acc: 0.9114 - val_loss: 0.2321 - val_acc: 0.9121\n",
      "\n",
      "Accuracy Score for Nerual Network is 91.209\n",
      "\n",
      "Nerual Network roc-auc is 0.956\n"
     ]
    }
   ],
   "source": [
    "#NEURAL NETWORK0\n",
    "\n",
    "## normalize the data\n",
    "X = df_test_over.iloc[:, :4].values\n",
    "\n",
    "\n",
    "\n",
    "#y-axis uses the variable outcome as its target variable\n",
    "y = df_test_over[\"Outcome\"].values\n",
    "\n",
    "#splitt the training data using 70% for training and 30% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(4,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "print(\"\\n************Start of Nerual Netwrok Summary*********\")\n",
    "model_1.summary()\n",
    "\n",
    "# Fit(Train) the Model\n",
    "model_1.compile(SGD(lr = .002), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=500)\n",
    "\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)\n",
    "\n",
    "y_pred_class_nn_1[:10]\n",
    "y_pred_prob_nn_1[:10]\n",
    "\n",
    "# Print model performance and plot the roc curve\n",
    "print('\\nAccuracy Score for Nerual Network is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)*100))\n",
    "print('\\nNerual Network roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.907407\n",
      "Confusion Matrix : \n",
      " [[34  5]\n",
      " [ 3 49]]\n",
      "Accuracy :  0.9120879120879121\n",
      "Sensitivity :  0.8717948717948718\n",
      "Specificity :  0.9423076923076923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, roc_auc_score, roc_curve, accuracy_score\n",
    "precision = precision_score(y_test, y_pred_class_nn_1)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "cm=confusion_matrix(y_test, y_pred_class_nn_1)\n",
    "print('Confusion Matrix : \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nerual Network AUCROC is 0.956\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_pred_prob_nn_1)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic for Nerual Network')\n",
    "plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],linestyle='--')\n",
    "plt.axis('tight')\n",
    "plt.grid(True)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rates')\n",
    "plt.savefig('nnroc.png')\n",
    "\n",
    "\n",
    "print('\\nNerual Network AUCROC is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "\n",
    "#create a confusion matrix to visually display the postive and negative prediction rate\n",
    "cmrf=confusion_matrix(y_test,y_pred_class_nn_1)\n",
    "conf_matrixrf=pd.DataFrame(data=cmrf,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.title(\"CONFUSION MATRIX FOR RANDOM FOREST\")\n",
    "sns.heatmap(conf_matrixrf, annot=True,fmt='d',cmap=\"YlGnBu\")\n",
    "plt.savefig('cmrfe.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVM is: 93.4065934065934\n",
      "\n",
      "SVM Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        44\n",
      "           1       0.96      0.91      0.93        47\n",
      "\n",
      "    accuracy                           0.93        91\n",
      "   macro avg       0.93      0.93      0.93        91\n",
      "weighted avg       0.94      0.93      0.93        91\n",
      "\n",
      "\n",
      "SVM AUCROC is 0.935\n",
      "Confusion Matrix for LOGISTICAL REGRESSION: \n",
      " [[42  2]\n",
      " [ 4 43]]\n",
      "Accuracy :  0.9340659340659341\n",
      "Sensitivity :  0.9545454545454546\n",
      "Specificity :  0.9148936170212766\n",
      "Precision: 0.955556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import support vector classifier \n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\" \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the epileptic data: 70%training||30% testing using the SKLearn train_test_Split\n",
    "x_train, x_test,y_train,y_test = train_test_split(df_test_over.drop('Outcome',axis=1),df_test_over['Outcome'],\n",
    "                                                  test_size=0.20,random_state=321)\n",
    "\n",
    "clf = SVC(kernel='linear', C = 0.1)\n",
    "\n",
    "# fitting x samples and y classes \n",
    "clf.fit(x_train,y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "\n",
    "\n",
    "#Evaluation of performance metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Accuracy score for SVM is:\",accuracy_score(y_test, predictions)*100)\n",
    "print(\"\\nSVM Classification Report\\n\")\n",
    "print(classification_report(y_test,predictions))\n",
    "print('\\nSVM AUCROC is {:.3f}'.format(roc_auc_score(y_test,predictions)))\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test, predictions)\n",
    "print('Confusion Matrix for LOGISTICAL REGRESSION: \\n', cm)\n",
    "\n",
    "total1=sum(sum(cm))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm[0,0]+cm[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "print('Sensitivity : ', sensitivity1)\n",
    "\n",
    "specificity1 = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "precision = precision_score(y_test, predictions)\n",
    "print('Precision: %f' % precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x274bf378908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAHwCAYAAAC2blbYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dfbhmdV3v8feHGQo4g5HMpDgwjDaURywRdoR5nSLSzgB6qLTSk0KaZ8IHnDp2OGYeGbowqytNHkwOxxQwDh7LhwyB0pQUC20P8ShWO0MZQRmGGBwZzYHv+eNeozeb/TTjXvvmt/f7dV33Nevht9b67nXN3p/791vrvleqCkmS1J59Rl2AJEnaO4a4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENcGrEkO5I8aZY2a7p2yxaqrke7uZy3no57TZKXLfRxpakY4tIsktyeZGcXGl9J8q4kK/ZyX48IgKpaUVWfn2m7qvpi1+7BvTnunkhSSdbNsP6QJB9KcmfXdu0e7n/4fO5+PWGWbfbqvEmLnSEuzc1zq2oFcDTwI8Dr92TjDCyW37eHgKuB530H+3huF8K7X3fOU23SkrJY/qhIC6KqvgRcBTw1yfcmuSLJ1iT/1k0furtt13t8Y5JPAQ8A7wb+E3BB1/u8oGv3rZ5vkv2TvDnJF5JsT3Jtt2xt12750L7flOQzXbs/T/LYoWP/aZIvd+s+keTIoXUXJ3lbkg8n+WqSTyf5/m7dJ7pmN3Y1/uIU5+ArVfVHwN/P13md6VwmeeMcztvBSf4iyf1J/j7JOUmu7dY97NwNnb+XDc2/NMlt3bH/MsnhQ+ueneRz3bm8AMh8/dzSd8oQl/ZAksOAk4B/YPD78y7gcGANsBO4YNImLwY2AAcCvwx8EnhV1/t81RSH+APgGODHgMcCZzLo+U7lVOClwBOAXcB5Q+uuAo4Avg+4Hrhs0rYvBM4GvheYAN4IUFU/3q1/Wlfj/5vm2NNK8tokV+zhZtOey6r6LWY/b28DvgY8Hjite8213p8BXgf8HLCqO9bl3bqVwPsYjLysBP4FeOYe/mxSb5bP3kQS8MEku4DtwIeB36mqnQz+wAPf6jF+fNJ2F1fVrUNtpj1AN9z+UuC4rscP8LczbPfuqrqlW/+/gBuSnFZVD1bVO4f2uwn4tyTfU1Xbu8Xvr6rPdOsvA94yy88/Z1X1u3Notvt8AlxTVT/D7OdySt3Nfs8DnlpVDwCfTXIJcPwcS/5V4E1VdVu3v98BXtf1xn8C+GxV/Vm37q3Aa+a4X6l3hrg0Nz9TVR8dXpDkAOAPgfUMerQAByZZNnQD2h17cIyVwH4MentzMbzvLwD7AiuT3MOgZ/3zDHqWu3vyKxm8CQH48tC2DwB7daPed+Bh53OO53I6qxj8LRs+H3ty3g8Hzk3y5qFlAVYzGOX41r6qqpLsyb6lXjmcLu291wA/CPxoVT0G2D0UPdxtnvyYwJkeG3gP8HXg++d4/MOGptcA3+z28V+BU4BnAd8DrJ2irkeb2c7lTOdtK4PLCYcOLRs+N1/r/j1gaNnjh6bvAH61qg4aeu1fVX8L3DW8rwyGRIb3LY2UIS7tvQMZXLu9r7up7Kw5bPMVYMrPNlfVQ8A7gbckeUKSZUmekeS7p9nXi5I8pevF/jbwZ12v9UDgG8A2BsH1O3v0U81Q425J9gN21/Xd3fx3YrZzOdN5exB4P7ApyQFJnszgfoHd67cCX2JwvpYleSkPf6N0IfCbu2/+S/I9SX6+W/dh4MgkP9fdGPdqHv4GQBopQ1zae28F9mfQ+72OwceuZnMu8PzuLujzplj/G8DNDO78vhf4Pab/PX03cDGDofH9GAQMwKUMhte/BHy2q21PbAIuSXJfkl+Yps1OYEc3/bluHoAkr0ty1R4ec7ZzOdt5exWDUYcvMzgvlzN4I7PbfwP+B4M3NkfS3WsAUFUfYHCe35PkfuAW4MRu3T0MLkv8brftEcCn9vBnk3qTqplGqSQ9GiW5BviTqnrHqGt5NErye8Djq2rOd6lLLbInLql5SZ6c5IczcCzwK8AHRl2X1DfvTpe0GBzIYAj9CcDdwJuBPx9pRdICcDhdkqRGOZwuSVKjDHFJkhrV3DXxlStX1tq1a0ddhiRJC2bz5s33VNWqycubC/G1a9cyPj4+6jIkSVowSb4w1XKH0yVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDWq1xBPcnuSm5PckOQRzw/NwHlJJpLclOToPuvRaI2Pj3PCCSewefPmUZciLSnbtm3j1a9+Ndu2bRt1KZpnC9ET/8mqOqqqxqZYdyJwRPfaALx9AerRiGzatImHHnqIs846a9SlSEvKJZdcws0338yll1466lI0z0Y9nH4KcGkNXAcclOSQEdekHoyPj7Njxw4AduzYYW9cWiDbtm3j6quvpqq4+uqr7Y0vMn2HeAF/lWRzkg1TrF8N3DE0v6VbpkVm06ZND5u3Ny4tjEsuuYSHHnoIgAcffNDe+CLTd4g/s6qOZjBs/sokPz5pfabYpiYvSLIhyXiS8a1bt/ZRp3q2uxc+3bykfnz0ox9l165dAOzatYuPfOQjI65I86nXEK+qO7t/7wY+ABw7qckW4LCh+UOBO6fYz0VVNVZVY6tWreqrXPVoxYoVM85L6seznvUsli9fDsDy5ct59rOfPeKKNJ96C/Ek/yHJgbungZ8GbpnU7EPAqd1d6scB26vqrr5q0uhMHk4/++yzR1OItMScdtpp7LPP4E/9smXLOPXUU0dckeZTnz3xxwHXJrkR+Azw4aq6OsnpSU7v2lwJfB6YAP4P8Ioe69EIjY2Nfav3vWLFCo455pgRVyQtDQcffDDr168nCevXr+fggw8edUmaR8v72nFVfR542hTLLxyaLuCVfdWgR5dNmzZx5pln2guXFthpp53G7bffbi98EcogR9sxNjZW4+OP+N4YSZIWrSSbp/q+lVF/TlySJO0lQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjeg/xJMuS/EOSK6ZYd3yS7Ulu6F5v6LseSZIWi+ULcIyNwG3AY6ZZ/8mqes4C1CFJ0qLSa088yaHAycA7+jyOJElLUd/D6W8FzgQemqHNM5LcmOSqJEf2XI8kSYtGbyGe5DnA3VW1eYZm1wOHV9XTgPOBD06zrw1JxpOMb926tYdqJUlqT5898WcC/yXJ7cB7gBOS/Mlwg6q6v6p2dNNXAvsmWTl5R1V1UVWNVdXYqlWreixZkqR29BbiVfWbVXVoVa0FXgB8rKpeNNwmyeOTpJs+tqtnW181SZK0mCzE3ekPk+R0gKq6EHg+8PIku4CdwAuqqha6JkmSWpTWMnNsbKzGx8dHXYYkSQsmyeaqGpu83G9skySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRvYd4kmVJ/iHJFVOsS5LzkkwkuSnJ0X3XI0nSYrEQPfGNwG3TrDsROKJ7bQDevgD1SJK0KPQa4kkOBU4G3jFNk1OAS2vgOuCgJIf0WZMkSYvF8p73/1bgTODAadavBu4Ymt/SLbur57qad/755zMxMTHqMvbIl770JQBWr1494krmbt26dZxxxhmjLmNRO//887n66qtHXcacPfDAA1TVqMtYEpJwwAEHjLqMOVu/fv2C/73orSee5DnA3VW1eaZmUyx7xG9Hkg1JxpOMb926dd5q1MLauXMnO3fuHHUZkrRopK93lEneBLwY2AXsBzwGeH9VvWiozf8Grqmqy7v5fwSOr6ppe+JjY2M1Pj7eS83q18aNGwE499xzR1yJJLUlyeaqGpu8vLeeeFX9ZlUdWlVrgRcAHxsO8M6HgFO7u9SPA7bPFOCSJOnb+r4m/ghJTgeoqguBK4GTgAngAeAlC12PJEmtWpAQr6prgGu66QuHlhfwyoWoQZKkxcZvbJMkqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNaq3EE+yX5LPJLkxya1Jzp6izfFJtie5oXu9oa96JGmpGh8f54QTTmDz5s2jLkXzrM+e+DeAE6rqacBRwPokx03R7pNVdVT3+u0e65GkJWnTpk089NBDnHXWWaMuRfOstxCvgR3d7L7dq/o6niTpkcbHx9mxY/CneMeOHfbGF5ler4knWZbkBuBu4CNV9ekpmj2jG3K/KsmRfdYjSUvNpk2bHjZvb3xx6TXEq+rBqjoKOBQ4NslTJzW5Hji8G3I/H/jgVPtJsiHJeJLxrVu39lmyJC0qu3vh082rbQtyd3pV3QdcA6yftPz+3UPuVXUlsG+SlVNsf1FVjVXV2KpVqxaiZElaFFasWDHjvNrW593pq5Ic1E3vDzwL+NykNo9Pkm762K6ebX3VJElLzeTh9LPPfsQHhdSw5T3u+xDgkiTLGITze6vqiiSnA1TVhcDzgZcn2QXsBF5QVd78JknzZGxsjBUrVrBjxw5WrFjBMcccM+qSNI96C/Gqugl4+hTLLxyavgC4oK8aJEmD3viZZ55pL3wR6rMnLkl6FBgbG+NjH/vYqMtQD/zaVUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqOWz7QyyWNnWl9V985vOZIkaa5mDHFgM1BAgDXAv3XTBwFfBJ7Ya3WSJGlaMw6nV9UTq+pJwF8Cz62qlVV1MPAc4P0LUaAkSZraXK+J/0hVXbl7pqquAn6in5IkSdJczDacvts9SV4P/AmD4fUXAdt6q0qSJM1qrj3xFwKrgA90r1XdMkmSNCJz6ol3d6FvTLKiqnb0XJMkSZqDOfXEk/xYks8Cn+3mn5bkj3qtTJIkzWiuw+l/CPxnuuvgVXUj8ON9FSVJkmY3529sq6o7Ji16cJ5rkSRJe2Cud6ffkeTHgEryXcCrgdv6K0uSJM1mrj3x04FXAquBLcBR3bwkSRqRud6dfg/wSz3XIkmS9sCcQjzJuxh8ycvDVNVL570iSZI0J3O9Jn7F0PR+wM8Cd85/OZIkaa7mOpz+vuH5JJcDH+2lIkmSNCdz/ojZJEcweDTptJLsl+QzSW5McmuSs6dokyTnJZlIclOSo/eyHkmSlpy5XhP/Kt9+rngBXwb+5yybfQM4oap2JNkXuDbJVVV13VCbExm8ITgC+FHg7d2/kiRpFnMdTj9wT3dcVQXs/p71fbvX5JvjTgEu7dpel+SgJIdU1V17erzv1Mte9jLuumvBD7uk7Ny5E4CTTz55xJUsbocccgjveMc7Rl2GpAUw1574VMPc24EvVNWuGbZbBmwG1gFvq6pPT2qyGhj+Jrgt3bKHpWmSDcAGgDVrZhzF32v33XcfO772ACyb671+2mPdW7gdX//30daxmD24i/vuu2/UVUhaIHNNrD8CjgZuYjCk/kPAjcDBSU6vqr+aaqOqehA4KslBwAeSPLWqbhlqkqk2m2I/FwEXAYyNjT1i/XxYvXo1X/7GcnY++aQ+di8tiP0/dyWrVz9u1GVIWiBzvbHtduDpVTVWVccw+Ma2W4BnAb8/28ZVdR9wDbB+0qotwGFD84fiR9ckSZqTuYb4k6vq1t0zVfVZBqH++ek2SLKq64GTZH8Ggf+5Sc0+BJza3aV+HLB9FNfDJUlq0VyH0/8xyduB93Tzvwj8U5LvBr45zTaHAJd018X3Ad5bVVckOR2gqi4ErgROAiaAB4CX7N2PIUnS0jPXEP9l4BXArzG4jn0t8BsMAvwnp9qgqm4Cnj7F8guHpgsfpCJJ0l6Z60fMdgJv7l6T7ZhimSRJ6tmMIZ7kvVX1C0luZuq7xn+4t8okSdKMZuuJb+z+fU7fhUiSpD0z493pQ3eKv6KqvjD8YnCNXJIkjchcP2L27CmWnTifhUiSpD0z2zXxlzPocT8pyU1Dqw4EPtVnYZIkaWazXRP/v8BVwJuA1w4t/2pV3dtbVZIkaVYzhnhVbWfwoJMXAiT5PmA/YEWSFVX1xf5LlCRJU5nTNfEkz03yz8C/An/D4LvUr+qxLkmSNIu53th2DnAc8E9V9UTgp/CauCRJIzXXEP9mVW0D9kmyT1V9nMGTzCRJ0ojM9bvT70uyAvgEcFmSu4Fd/ZUlSZJmM2NPPMmabvIUBk8Z+3XgauBfgOf2W5okSZrJbD3xDwJHV9XXkryvqp4HXLIAdUmSpFnMdk08Q9NP6rMQSZK0Z2YL8ZpmWpIkjdhsw+lPS3I/gx75/t003XxV1WN6rU6SJE1rtm9sW7ZQhUiSpD0z18+JS5KkRxlDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqVG8hnuSwJB9PcluSW5NsnKLN8Um2J7mhe72hr3okSVpslve4713Aa6rq+iQHApuTfKSqPjup3Ser6jk91iFJ0qLUW0+8qu6qquu76a8CtwGr+zqeJElLzYJcE0+yFng68OkpVj8jyY1Jrkpy5ELUI0nSYtDncDoASVYA7wN+rarun7T6euDwqtqR5CTgg8ARU+xjA7ABYM2aNT1XLElSG3rtiSfZl0GAX1ZV75+8vqrur6od3fSVwL5JVk7R7qKqGquqsVWrVvVZsiRJzejz7vQAfwzcVlVvmabN47t2JDm2q2dbXzVJkrSY9Dmc/kzgxcDNSW7olr0OWANQVRcCzwdenmQXsBN4QVVVjzVJkrRo9BbiVXUtkFnaXABc0FcNkiQtZn5jmyRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUb2FeJLDknw8yW1Jbk2ycYo2SXJekokkNyU5uq96JGmpmpiY4OSTT2ZiYmLUpWie9dkT3wW8pqr+I3Ac8MokT5nU5kTgiO61AXh7j/VI0pJ0zjnn8LWvfY1zzjln1KVonvUW4lV1V1Vd301/FbgNWD2p2SnApTVwHXBQkkP6qkmSlpqJiQluv/12AG6//XZ744vM8oU4SJK1wNOBT09atRq4Y2h+S7fsroWoa7JlD9zL/p+7chSHXhL2+fr9ADy032NGXMniteyBe4HHjboMPYpM7n2fc845XHzxxaMpRvOu9xBPsgJ4H/BrVXX/5NVTbFJT7GMDg+F21qxZM+81Aqxbt66X/erbJia+CsC6Jxky/Xmc/5f1MLt74dPNq229hniSfRkE+GVV9f4pmmwBDhuaPxS4c3KjqroIuAhgbGzsESE/H84444w+dqshGzcO7m0899xzR1yJtHSsXbv2YcG9du3akdWi+dfn3ekB/hi4rareMk2zDwGndnepHwdsr6qRDKVL0mL0+te/fsZ5ta3PnvgzgRcDNye5oVv2OmANQFVdCFwJnARMAA8AL+mxHklactatW/et3vjatWu93LLI9BbiVXUtU1/zHm5TwCv7qkGSNOh9b9y40V74IrQgd6dLkkZn3bp1fPjDHx51GeqBX7sqSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWpUbyGe5J1J7k5yyzTrj0+yPckN3esNfdUiSdJitLzHfV8MXABcOkObT1bVc3qsQZKkRau3nnhVfQK4t6/9S5K01I36mvgzktyY5KokR464FkmSmtLncPpsrgcOr6odSU4CPggcMVXDJBuADQBr1qxZuAolSXoUG1lPvKrur6od3fSVwL5JVk7T9qKqGquqsVWrVi1onZIkPVqNLMSTPD5Juulju1q2jaoeSZJa09twepLLgeOBlUm2AGcB+wJU1YXA84GXJ9kF7AReUFXVVz2SJC02vYV4Vb1wlvUXMPgImiRJ2gujvjtdkiTtJUNckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDXKEJckqVGGuCRJjTLEJUlqlCEuSVKjDHFJkhpliEuS1ChDXJKkRhnikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIa1VuIJ3lnkruT3DLN+iQ5L8lEkpuSHN1XLZIkLUZ99sQvBtbPsP5E4IjutQF4e4+1SJK06PQW4lX1CeDeGZqcAlxaA9cBByU5pK96JElabJaP8NirgTuG5rd0y+4aTTltOf/885mYmBh1GXtkd70bN24ccSVzt27dOs4444xRlyFJUxpliGeKZTVlw2QDgyF31qxZ02dN6tH+++8/6hIkaVEZZYhvAQ4bmj8UuHOqhlV1EXARwNjY2JRBv9TYO5QkjfIjZh8CTu3uUj8O2F5VDqVLkjRHvfXEk1wOHA+sTLIFOAvYF6CqLgSuBE4CJoAHgJf0VYskSYtRbyFeVS+cZX0Br+zr+JIkLXZ+Y5skSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsQlSWqUIS5JUqMMcUmSGmWIS5LUKENckqRGGeKSJDUqg8d6tyPJVuALo65De20lcM+oi5CWIH/32nZ4Va2avLC5EFfbkoxX1dio65CWGn/3FieH0yVJapQhLklSowxxLbSLRl2AtET5u7cIeU1ckqRG2ROXJKlRhrgWRJL1Sf4xyUSS1466HmmpSPLOJHcnuWXUtWj+GeLqXZJlwNuAE4GnAC9M8pTRViUtGRcD60ddhPphiGshHAtMVNXnq+rfgfcAp4y4JmlJqKpPAPeOug71wxDXQlgN3DE0v6VbJkn6DhjiWgiZYpkfi5Ck75AhroWwBThsaP5Q4M4R1SJJi4YhroXw98ARSZ6Y5LuAFwAfGnFNktQ8Q1y9q6pdwKuAvwRuA95bVbeOtippaUhyOfB3wA8m2ZLkV0Zdk+aP39gmSVKj7IlLktQoQ1ySpEYZ4pIkNcoQlySpUYa4JEmNMsSlRSjJoUn+PMk/J/mXJOd2n9GfaZvXLVR9kuaHIS4tMkkCvB/4YFUdAfwAsAJ44yybGuJSYwxxafE5Afh6Vb0LoKoeBH4deGmSVyS5YHfDJFckOT7J7wL7J7khyWXdulOT3JTkxiTv7pYdnuSvu+V/nWRNt/ziJG9P8vEkn0/yE91zrG9LcvHQ8X46yd8luT7JnyZZsWBnRVqEDHFp8TkS2Dy8oKruB74ILJ9qg6p6LbCzqo6qql9KciTwW8AJVfU0YGPX9ALg0qr6YeAy4Lyh3XwvgzcQvw78BfCHXS0/lOSoJCuB1wPPqqqjgXHgv8/HDywtVVP+QktqWpj6KXHTLZ/KCcCfVdU9AFW1+3nUzwB+rpt+N/D7Q9v8RVVVkpuBr1TVzQBJbgXWMnjwzVOATw1G/PkuBl8HKmkvGeLS4nMr8LzhBUkew+BJctt5+AjcftPsY66BP9zmG92/Dw1N755fDjwIfKSqXjiH/UqaA4fTpcXnr4EDkpwKkGQZ8GbgYuDzwFFJ9klyGHDs0HbfTLLv0D5+IcnB3T4e2y3/WwZPoQP4JeDaPajrOuCZSdZ1+zwgyQ/s6Q8n6dsMcWmRqcFTjX4W+Pkk/wz8E/B1Bneffwr4V+Bm4A+A64c2vQi4Kcll3VPm3gj8TZIbgbd0bV4NvCTJTSMAgscAAABPSURBVMCL+fa18rnUtRX4ZeDybvvrgCfv7c8pyaeYSZLULHvikiQ1yhCXJKlRhrgkSY0yxCVJapQhLklSowxxSZIaZYhLktQoQ1ySpEb9f7uc2GjjyrVjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.title(\"Participant 1: Fatigued\")\n",
    "sns.boxplot(x='Outcome', y='Fatigued', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
